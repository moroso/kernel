/* interface and implementation of the mailbox descriptor table, which translates
 * ipc descriptors to mailbox ids kernel-side.  There is one descriptor table
 * per proc
 *
 * TODO descriptors should be kept in a hash table, so we don't have such
 * dreadful limits!
 *
 * Author Amanda M. Watson
*/

use structures::tcb::*;
use utils::stdlib::*;
use structures::schedule::*;
use structures::tcb_dir::*;
use locks::mutex::*;
use structures::mailbox::*;
use locks::lol_atomics::*;
use structures::proc::*;

const MB_TABLE_SIZE: u32 = 64; // number of descriptors in the table

/*** desc_table error codes ***/
/* system-internal error */
const DESC_ERR_INTERNAL: i32 = -2;
/* descriptor table is too full for more entries */
const DESC_ERR_FULL: i32 = -1;
/* invalid arguments to function */
const DESC_ERR_ARGS: i32 = -3;
/* operation on empty descriptor */
const DESC_ERR_EMPTY: i32 = -4;

struct mb_desc_table {
    desc_table: (*mailbox_t)[MB_TABLE_SIZE],
    mut: kmut,
}

/* Initializes an mb_desc_table; if there is a parent_proc, copies its
 * descriptor translations; otherwise, sets all table indices to null.
*/
fn mb_desc_init(table: *mb_desc_table, parent_proc: *proc) {
    kmut_init(&table->mut);
    if (parent_proc == null) {
        printf!("Parent proc null\n");
        mos_memset(&(table->desc_table)[0], 0, sizeof(*mailbox_t) * MB_TABLE_SIZE);
    } else {
        let i: u32;
        let child_table = table;
        let parent_table =  &(parent_proc->mb_desc);
        kmut_lock(&parent_table->mut);
        for (i = 0; i < MB_TABLE_SIZE; i+=1) {
            child_table->desc_table[i] = null;
            if (parent_table->desc_table[i] != null) {

                let mb = parent_table->desc_table[i];
                (child_table->desc_table)[i] = mb;
                inc_mb_ref(mb);
            }
        }
        kmut_unlock(&parent_table->mut);
    }
}

/* given a mailbox descriptor, returns corresponding entry in table.  returns
 * null if id cannot be a valid descriptor */
fn mb_desc_lookup(id: u32, table: *mb_desc_table) -> *mailbox_t {
    if (id >= MB_TABLE_SIZE || id < 0) {
        return null;
    }
    kmut_lock(&table->mut);
    let mb = table->desc_table[id];
    if (mb != null) {
        inc_mb_ref(mb);
    }
    kmut_unlock(&table->mut);
    mb
}

fn mb_end_lookup(mb: *mailbox_t) {
    dec_mb_ref(mb);
}

fn dec_mb_ref(mb: *mailbox_t) {
    if (mb == null) {
        return ();
    }
    let last_count = atomic_add(&mb->ref_count, -1);
    if (last_count == 1) {
        mb_destroy(mb);
    }
}

fn inc_mb_ref(mb: *mailbox_t) {
    if (mb == null) {
        return ();
    }
    atomic_add(&mb->ref_count, 1);
}

// add mailbox to descriptor table, return descriptor.  returns failure if all
// descriptor slots are taken
fn mb_table_add(table: *mb_desc_table, mb: *mailbox_t) -> i32 {
    if (mb == null) {
        return DESC_ERR_INTERNAL;
    }
    kmut_lock(&table->mut);
    let i: u32;
    for (i = 0; i < MB_TABLE_SIZE; i+=1) {
        if (table->desc_table[i] == null) {
            mb_entry_edit_unsafe(table, mb, i);
            kmut_unlock(&table->mut);
            return i as i32;
        }
    }
    kmut_unlock(&table->mut);
    DESC_ERR_FULL
}

fn mb_entry_edit_unsafe(table: *mb_desc_table, mb: *mailbox_t, desc: u32) {
    (table->desc_table)[desc] = mb;
    inc_mb_ref(mb);
}

fn mb_entry_edit(table: *mb_desc_table, mb: *mailbox_t, desc: u32) -> i32 {
    if (desc > MB_TABLE_SIZE) {
        return -1;
    }
    kmut_lock(&table->mut);
    mb_entry_edit_unsafe(table, mb, desc);
    kmut_unlock(&table->mut);
    0
}

// check to see if the descriptor corresponds to the mailbox before removing
fn mb_table_dead_entry_remove(desc: u32, mb: *mailbox_t) {
    let table = &(get_tcb()->proc->mb_desc);
    kmut_lock(&table->mut);
    if (table->desc_table[desc] == mb) {
        mb_table_remove_unsafe(table, desc);
    }
    kmut_unlock(&table->mut);

}

fn mb_table_remove(table: *mb_desc_table, desc: u32) {
    if (desc >= MB_TABLE_SIZE || desc < 0) {
        return ();
    }
    kmut_lock(&table->mut);
    mb_table_remove_unsafe(table, desc);
    kmut_unlock(&table->mut);
}

fn mb_table_remove_unsafe(table: *mb_desc_table, desc: u32) {
    if (desc >= MB_TABLE_SIZE || desc < 0) {
        return ();
    }
    let mb = table->desc_table[desc];
    if (mb != null) {
        if (mb->owner == get_tcb()->proc) {
            mb->active = false;
        }
        dec_mb_ref(mb);
    }
    table->desc_table[desc] = null;
}



// dec ref counts on all mailboxes in descriptor table.  free any that no
// longer have a reference
// NOTE if the only way references can be added is by repls and forwarding, we
// don't have to worry about references being added is we're the last one
fn mb_table_destroy(table: *mb_desc_table) {
    kmut_lock(&table->mut);
    let i: u32;
    for (i = 0; i < MB_TABLE_SIZE; i+=1) {
        mb_table_remove_unsafe(table, i);
    }
    kmut_unlock(&table->mut);
}

/* if old entry has no mailbox, returns with failure */
fn mb_move(table: *mb_desc_table, old_desc: u32, new_desc: u32) -> i32 {
    if (old_desc >= MB_TABLE_SIZE || new_desc >= MB_TABLE_SIZE) {
        return DESC_ERR_ARGS;
    }
    kmut_lock(&table->mut);
    if (table->desc_table[old_desc] == null) {
        kmut_unlock(&table->mut);
        return DESC_ERR_EMPTY;
    }
    mb_table_remove_unsafe(table, new_desc);
    table->desc_table[new_desc] = table->desc_table[old_desc];
    mb_table_remove_unsafe(table, old_desc);
    kmut_unlock(&table->mut);
    0
}
