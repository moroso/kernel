/** schedule.mb: contains implementation of scheduler functions
  * schedule() and unschedule() add and remove from the run queue.
  * scheduler_update is called to replace the currently running thread with the
  * next available off the run queue.
  * All scheduler operations will require preemption to be disabled
  * The run queue is an ordered list storing runnable
  * tcb's: tcbs are removed from the run queue in round robin (NOTE:
  * experimenting with scheduler policies sounds fun) order, with the latest tcb
  * denoted as "running" (a context switch will allow this new tcb to actually be
  * run).  TCB's are added to the run queue using schedule(), and removed with
  * unschedule().
  *
  * Rather than explicitly enabling/disabling interrupts, we choose to keep a
  * preemption flag; the running thread will only be preempted if the flag is
  * enabled (even if interrupts and enabled).
  *
  * // TODO set a flag that indicates thread missed interrupt with preemption
  * disabeld, and should switch
  *  // TODO assert that the current stack pointer corresponds to a value on the
  *  // running tcb's kstack
  * Author: Amanda M. Watson
 */

use consts::*;
use utils::list::*;
use structures::tcb::*;
use structures::VM::*;
use context::*;

extern fn disable_interrupts();
extern fn enable_interrupts();
extern fn get_cr3() -> u32; // hm.
extern fn get_eflags() -> u32;

/* Used in schedule() to denote whether a tcb should be added to the front
 * or the back of the run queue.
 */
const QUEUE_FRONT: u32 = 0;
const QUEUE_BACK: u32 = 1;

/* a data structure that manages run queue state */
struct rq_master {
    run_Q: list_head, // head of run queue
    running: *tcb, // actively running tcb
    preempt_enabled: bool, // whether preemption is enabled
    inter_flag: bool, // set if an interrupt is missed while preemption isn't
                      // enabled; interrupt occurs as soon as it gets re-enabled
    idle_tcb: *tcb,
}

/*
 * global data structure: controls the run queue of runnable tcb's,
 * denotes the tcb that is currently running, and whether preemption is enabled
 */
static run_queue: rq_master = rq_master {
run_Q: LIST_HEAD_INIT!(run_queue.run_Q),
           running: null,
           preempt_enabled: false,
           inter_flag: false,
           idle_tcb: null,
};

fn preempt_disable() {
    run_queue.preempt_enabled = false;
}

fn preempt_enable() {
    run_queue.preempt_enabled = true;
    // if we missed an interrupt while preemption was disabled,
    // let's take it now
    if (run_queue.inter_flag) {
        run_queue.inter_flag = false;
        scheduler_update(-1);
    }
}

fn cond_preempt_disable() -> bool {
    let is_pre_enable = is_preempt_enabled();
    if is_pre_enable {
        preempt_disable();
    };

    is_pre_enable
}


fn cond_preempt_enable(enable: bool) {
    if enable {
        preempt_enable();
    }
}
fn is_preempt_enabled() -> bool {
    run_queue.preempt_enabled
}

fn is_scheduled(tcb: *tcb) -> bool {

    if (run_queue.running == tcb) {
        return true;
    }
    if (!(tcb->link.next == null && tcb->link.prev == null)) {
        return true;
    }
    false
}

/*
 * add a tcb to the run queue: position should be QUEUE_FRONT if tcb should be run immediately,
 * QUEUE_BACK if it should patiently wait its turn in the queue.
 * Requires that tcb's state be set to NEW or RUNNABLE before insertion, so the
 * scheduler knows you really mean it.
 */
fn schedule(tcb: *tcb, pos: u32) {
    let enable = scheduler_lock();
    if (tcb->state == STATE_NEW) {
        set_state(tcb, STATE_RUNNABLE);
    }
    assert!(tcb->state == STATE_RUNNABLE);
    add_to_run_queue(tcb, pos);
    scheduler_unlock(enable);
}

fn add_to_run_queue(tcb: *tcb, pos: u32) {
    let enable = scheduler_lock();
    assert!(pos == QUEUE_FRONT || pos == QUEUE_BACK);
    assert!(tcb->state == STATE_RUNNABLE);
    if (pos == QUEUE_FRONT) {
        list_insert_head(&tcb->link, &run_queue.run_Q);
    } else {
        list_insert_tail(&tcb->link, &run_queue.run_Q);
    }
    scheduler_unlock(enable);
}

/* Remove a tcb from the run queue */
fn unschedule(tcb: *tcb) {
    assert!(!is_preempt_enabled());
    assert!(tcb->state == STATE_RUNNABLE);
    list_del(&tcb->link)
}

/* returns pointer to running tcb: later if we have aligned memory we can find it via TCB; for now, we keep
   a pointer in the scheduler */
fn get_tcb() -> *tcb {
    run_queue.running
}



fn are_ints_enabled() -> bool {
    if (get_eflags()&EF_IF == 1) {
        return true;
    }
    false
}

// TODO there is a bug if we mess around with interrupts.  This is cheap hack.
// pls fix
fn scheduler_unlock(enable: bool) {
    if enable {
        preempt_enable();
    }
}

fn scheduler_lock() -> bool {
    let ints_enabled = is_preempt_enabled();
    if ints_enabled {
        preempt_disable();
    };

    ints_enabled
}

fn run_tcb(new_running: *tcb) {
    assert!(new_running != null);
    assert!(new_running->state == STATE_RUNNABLE);
    run_queue.running = new_running;

    /*  if the address space has changed, load a new one */
    if ((get_cr3() as *u8) != (&(new_running->proc->PD.PT_entries) as *u8)) {
        load_PD(&(new_running->proc->PD));
    }
}

fn round_robin_remove() -> *tcb {
    let new_running = list_head_entry!(&run_queue.run_Q, tcb, link);
    if (new_running != null) {
        list_del(&(new_running->link));
    }
    new_running
}


/* Replaces the running thread with the next thread in the run queue, and then
 * switches to this latest thread.  optionally takes in a state to assign the
 * currently running thread (if a running tcb's state is changed to MUT or COND,
 * for instance, it won't be re-added to the run queue)
 * State is ignored if state == -1 */
fn scheduler_update(state: i32) {
    let enable = scheduler_lock();

    /* the head of the run queue, if non-null is to be run next.  a
     * non-round-robin scheduler would likely change this */
    let old_running: *tcb = get_tcb();
    let new_running: *tcb = round_robin_remove();

    /* if there are no other threads in the run queue, continue with current
       thread and do not update */
    if (new_running == null) {
        // there must be one thread running at all times
        assert!(old_running != null && (state as u32 == STATE_RUNNABLE
                    || state == -1));
        scheduler_unlock(enable);
        return ();
    } 

    /* if a state was provided, assign it to the running thread */
    if (state != -1) {
        assert!(old_running != null);
        old_running->state = state as u32;
    }

    if (old_running != null) {
        assert!(old_running->state != STATE_FP_RECV as u32);
        if (old_running->state == STATE_RUNNABLE) {
            add_to_run_queue(old_running, QUEUE_BACK);
        }

    }

    run_tcb(new_running);
    context_switch(old_running, new_running);

    scheduler_unlock(enable);
}

fn fp_recv(buffer: u32[NUM_REGS]) {
    let enable = scheduler_lock();

    let new_running: *tcb = round_robin_remove();
    let old_running: *tcb = get_tcb();

    // there should always be an idle process available
    assert!(new_running != null);
    assert!(old_running != null);
    assert!(old_running->state == STATE_RUNNABLE as u32);

    old_running->state = STATE_FP_RECV as u32;
    run_tcb(new_running);
    fastpath_recv(old_running, new_running, buffer);
    assert!(get_tcb()->state == STATE_RUNNABLE);
    scheduler_unlock(enable);
}


fn fp_send(tcb: *tcb, buffer: u32[NUM_REGS]) -> i32{
    let enable = scheduler_lock();
    assert!(tcb != null);

    if (tcb->state as i32 != STATE_FP_RECV) {
        printf!("%d\n", tcb->state);
    }

    let new_running: *tcb = tcb; 
    let old_running: *tcb = get_tcb();

    add_to_run_queue(old_running, QUEUE_BACK); 
    new_running->state = STATE_RUNNABLE;
    run_tcb(new_running);

    fastpath_send(old_running, new_running, buffer);

    scheduler_unlock(enable);
    0
}

