/** schedule.mb: contains implementation of scheduler functions
  * schedule() and unschedule() add and remove from the run queue.
  * scheduler_update is called to replace the currently running thread with the
  * next available off the run queue.
  * All scheduler operations will require preemption to be disabled
  * The run queue is an ordered list storing runnable
  * tcb's: tcbs are removed from the run queue in round robin (NOTE:
  * experimenting with scheduler policies sounds fun) order, with the latest tcb
  * denoted as "running" (a context switch will allow this new tcb to actually be
  * run).  TCB's are added to the run queue using schedule(), and removed with
  * unschedule().
  *
  * Rather than explicitly enabling/disabling interrupts, we choose to keep a
  * preemption flag; the running thread will only be preempted if the flag is
  * enabled (even if interrupts and enabled).
  *
  * // TODO set a flag that indicates thread missed interrupt with preemption
  * disabeld, and should switch
  *  // TODO assert that the current stack pointer corresponds to a value on the
  *  // running tcb's kstack
  * Author: Amanda M. Watson
 */

use consts::*;
use utils::list::*;
use structures::tcb::*;
use structures::VM::*;
use context::context_switch;

extern fn disable_interrupts();
extern fn enable_interrupts();
extern fn get_cr3() -> u32; // hm.
extern fn get_eflags() -> u32;

/* Used in schedule() to denote whether a tcb should be added to the front
 * or the back of the run queue.
 */
const QUEUE_FRONT: u32 = 0;
const QUEUE_BACK: u32 = 1;

/* a data structure that manages run queue state */
struct rq_master {
    run_Q: list_head, // head of run queue
    running: *tcb, // actively running tcb
    preempt_enabled: bool, // whether preemption is enabled
    inter_flag: bool, // set if an interrupt is missed while preemption isn't 
                      // enabled; interrupt occurs as soon as it gets re-enabled 
    idle_tcb: *tcb,
}

/*
 * global data structure: controls the run queue of runnable tcb's,
 * denotes the tcb that is currently running, and whether preemption is enabled
 */
static run_queue: rq_master = rq_master {
run_Q: LIST_HEAD_INIT!(run_queue.run_Q),
           running: null,
           preempt_enabled: false,
           inter_flag: false,
           idle_tcb: null,
};

fn preempt_disable() {
    run_queue.preempt_enabled = false;
}

fn preempt_enable() {
    run_queue.preempt_enabled = true;
    enable_interrupts();
    // if we missed an interrupt while preemption was disabled, 
    // let's take it now
    if (run_queue.inter_flag) {
        run_queue.inter_flag = false;
        scheduler_update(-1);
    }
}


fn is_preempt_enabled() -> bool {
    run_queue.preempt_enabled
}

fn is_scheduled(tcb: *tcb) -> bool {

    if (run_queue.running == tcb) {
        return true;
    }
    if (!(tcb->link.next == null && tcb->link.prev == null)) {
        return true;
    }       
    false
}

/*
 * add a tcb to the run queue: position should be QUEUE_FRONT if tcb should be run immediately,
 * QUEUE_BACK if it should patiently wait its turn in the queue.
 * Requires that tcb's state be set to NEW or RUNNABLE before insertion, so the
 * scheduler knows you really mean it.
 */
fn schedule(tcb: *tcb, pos: u32) {
    let enable = cond_preempt_disable();
    assert!(pos == QUEUE_FRONT || pos == QUEUE_BACK);

    if (tcb->state == STATE_NEW) {
        set_state(tcb, STATE_RUNNABLE);
    }
    assert!(tcb->state == STATE_RUNNABLE);
    if (pos == QUEUE_FRONT) {
        list_insert_head(&tcb->link, &run_queue.run_Q);
        cond_preempt_enable(enable);

        return ();
    }
    if (pos == QUEUE_BACK) {
        list_insert_tail(&tcb->link, &run_queue.run_Q);
        cond_preempt_enable(enable);

        return ();
    }
    // pos should specify either the front or back position
    assert!(false);
    cond_preempt_enable(enable);

}

/* Remove a tcb from the run queue */
fn unschedule(tcb: *tcb) {
    assert!(!is_preempt_enabled());
    assert!(tcb->state == STATE_RUNNABLE);
    list_del(&tcb->link)
}

/* returns pointer to running tcb: later if we have aligned memory we can find it via TCB; for now, we keep
   a pointer in the scheduler */
fn get_tcb() -> *tcb {
    run_queue.running
}

fn cond_preempt_disable() -> bool {
    let is_pre_enable = is_preempt_enabled();
    if is_pre_enable {
        preempt_disable();
    };

    is_pre_enable
}


fn cond_preempt_enable(enable: bool) {
    if enable {
        preempt_enable();
    }
}

fn are_ints_enabled() -> bool {
    if (get_eflags()&EF_IF == 1) {
        return true;
    }
    false
}

fn scheduler_unlock(enable: bool) {
    if enable {
        enable_interrupts();
    }
}

fn scheduler_lock() -> bool {
    let ints_enabled = are_ints_enabled();
    if ints_enabled {
        disable_interrupts();
    };

    ints_enabled
}

/* Replaces the running thread with the next thread in the run queue, and then
 * switches to this latest thread.  optionally takes in a state to assign the
 * currently running thread (if a running tcb's state is changed to MUT or COND,
 * for instance, it won't be re-added to the run queue)
 * State is ignored if state == -1 */
fn scheduler_update(state: i32) {
    let enable = cond_preempt_disable();

    /* the head of the run queue, if non-null is to be run next.  a
     * non-round-robin scheduler would likely change this */
    let new_running: *tcb = list_head_entry!(&run_queue.run_Q, tcb, link);
    let old_running: *tcb = get_tcb();

    /* if there are no other threads in the run queue, continue with current
       thread and do not update */
    if (new_running == null) {

        // there must be one thread running at all times
        
        if (old_running == null) {
            assert!(run_queue.idle_tcb != null);
            new_running = run_queue.idle_tcb;
        } else {
            cond_preempt_enable(enable);
            return ();
        }
    }

    /* if a state was provided, assign it to the running thread */
    if (state != -1) {
        assert!(old_running != null);
        old_running->state = state as u32;
    }

    if (old_running != null) {
        assert!(run_queue.idle_tcb != null);
        if (old_running->state == STATE_RUNNABLE && run_queue.idle_tcb !=
                old_running) {
            list_insert_tail(&old_running->link, &(run_queue.run_Q));
            run_queue.running = null;
        }

        if (old_running->state == STATE_DEAD) {
            printf1_("thread %d has perished\n", old_running->tid);
        }
    }

    assert!(new_running != null);
    list_del(&(new_running->link));
    run_queue.running = new_running;

    /* if the address space has changed, load a new one */
    if ((get_cr3() as *u8) != (&(new_running->proc->PD.PT_entries) as *u8)) {
        load_PD(&(new_running->proc->PD));
    }

    context_switch(old_running, new_running);

    cond_preempt_enable(enable);
}
