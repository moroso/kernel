/** schedule.mb: contains implementation of scheduler functions
  * schedule() and unschedule() add and remove from the run queue.
  * scheduler_update is called to replace the currently running thread with the
  * next available off the run queue.
  * All scheduler operations will require preemption to be disabled
  * The run queue is an ordered list storing runnable
  * tcb's: tcbs are removed from the run queue in round robin (NOTE:
  * experimenting with scheduler policies sounds fun) order, with the latest tcb
  * denoted as "running" (a context switch will allow this new tcb to actually be
  * run).  TCB's are added to the run queue using schedule(), and removed with
  * unschedule().
  *
  * Rather than explicitly enabling/disabling interrupts, we choose to keep a
  * preemption flag; the running thread will only be preempted if the flag is
  * enabled (even if interrupts and enabled).
  * 
  *
  * Author: Amanda M. Watson
 */

/* Used in schedule() to denote whether a tcb should be added to the front
 * or the back of the run queue.
 */
const FRONT: u32 = 0;
const BACK: u32 = 1;

/* a data structure that manages run queue state */
struct rq_master {
run_Q: list_head, // head of run queue
           running: *tcb, // actively running tcb
           preempt_enabled: bool, // whether preemption is enabled
}

/*
 * global data structure: controls the run queue of runnable tcb's,
 * denotes the tcb that is currently running, and whether preemption is enabled
 */
static run_queue: rq_master = rq_master {
run_Q: LIST_HEAD_INIT!(run_queue.run_Q),
           running: null,
           preempt_enabled: false
};

fn preempt_disable() {
    run_queue.preempt_enabled = false;
}

fn preempt_enable() {
    run_queue.preempt_enabled = true;
}

fn is_preempt_enabled() -> bool {
    run_queue.preempt_enabled
}

/*
 * add a tcb to the run queue: position should be FRONT if tcb should be run immediately,
 * BACK if it should patiently wait its turn in the queue.
 * Requires that tcb's state be set to NEW or RUNNABLE before insertion, so the
 * scheduler knows you really mean it.
 */
fn schedule(tcb: *tcb, pos: u32) {
    assert(!is_preempt_enabled());
    assert(pos == FRONT || pos == BACK);

    if (tcb->state == STATE_NEW) {
        set_state(tcb, STATE_RUNNABLE);
    }
    assert(tcb->state == STATE_RUNNABLE);
    if (pos == FRONT) {
        list_insert_head(&tcb->link, &run_queue.run_Q);
        return ();
    }
    if (pos == BACK) {
        list_insert_tail(&tcb->link, &run_queue.run_Q);
        return ();
    }
    // pos should specify either the front or back position
    assert(false);
}

/* Remove a tcb from the run queue */
fn unschedule(tcb: *tcb) {
    assert(!is_preempt_enabled());
    assert(tcb->state == STATE_RUNNABLE);
    list_del(&tcb->link)
}

/* returns pointer to running tcb: later if we have aligned memory we can find it via TCB; for now, we keep
   a pointer in the scheduler */
fn get_tcb() -> *tcb {
    run_queue.running
}

fn cond_preempt_disable() -> bool {
    let is_pre_enable = is_preempt_enabled();
    if is_pre_enable {
        preempt_disable();
    };

    is_pre_enable
}
fn cond_preempt_enable(enable: bool) {
    if enable {
        preempt_enable();
    }
}

/* Replaces the running thread with the next thread in the run queue, and then
 * switches to this latest thread.  optionally takes in a state to assign the
 * currently running thread (if a running tcb's state is changed to MUT or COND,
 * for instance, it won't be re-added to the run queue)
 * State is ignored if state == -1 */
fn scheduler_update(state: i32) {
    /* preemption must be disabled for this section, but we also want to enable
     * nesting */
    let enable = cond_preempt_disable();

    /* the head of the run queue, if non-null is to be run next.  a
     * non-round-robin scheduler would likely change this */
    let new_running: *tcb = list_head_entry!(&run_queue.run_Q, tcb, link);
    let old_running: *tcb = get_tcb();

    /* if there are no other threads in the run queue, continue with current
       thread and do not update */
    if (new_running == null) {
        cond_preempt_enable(enable);
        return ();
    }

    /* if a state was provided, assign it to the running thread */
    if (state != -1) {
        assert(old_running != null);
        old_running->state = state as u32;
    }

    if (old_running != null) {
        // TODO assert that the current stack pointer corresponds to a value on the
        // running tcb's kstack
        // TODO insert into sleep queue?
        if (old_running->state == STATE_RUNNABLE) {
            // we may want this to be "running" instead
            list_insert_tail(&old_running->link, &(run_queue.run_Q));
            run_queue.running = null;
        }
    }

    assert(new_running != null);
    list_del(&(new_running->link));
    run_queue.running = new_running;

    /* if the address space has changed, load a new one */
    if ((get_cr3() as *u8) != (&(new_running->proc->PD.PT_entries) as *u8)) {
        load_PD(&(new_running->proc->PD));   
    }

    context_switch(old_running, new_running);

    cond_preempt_enable(enable);
}
