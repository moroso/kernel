/**
 * VM.mb: contains implementations of kernel virtual memory operations
 * A static region of the address space represents kernel memory, and is
 * direct-mapped into a PD.  The rest is user memory, and is mapped into a PD at
 * will.  Pages can be mapped in as a 4KB page table entry (2-level), or a 4MB page directory
 * entry (1-level).  There are plans for an elaborate locking scheme, but one does not
 * currently exist.
 * XXX TODO TLB invalidation, freeing frames
 * Author: Amanda M. Watson
*/

const PAGES_PER_PT: u32 = 1024;
const PTS_PER_DIR: u32 = 1024;

/* initial page directory. contains only kernel memory */
static init_PD: *PD_t = null;
/* contains ref counts for each frame */
static frame_table: *u32 = null;

/* used to distinguish between 4KB and 4MB page sizes */
// size of a kilobyte
const KB: u32 = 1024;
// size of a megabyte
const MB: u32 = KB * 1024;

// offset into address where page directory entry is located
const PAGE_DIR_OFFSET: u32 = 22;
// offset into address where page table entry is located
const PAGE_TABLE_OFFSET: u32 = 12;
/* masks for interpreting portions of a logical address */
const PD_MASK: u32 = 0xFFC00000;
const PT_MASK: u32 = 0x003FF000;
const FLAG_MASK: u32 = ~((PD_MASK) | (PT_MASK));

/* in both tables and entries.  if 0, means that all attempts
 * at access should result in a page fault */
const PRESENT_BIT: u32 = 1;
/* if set, page is writable.
 * means different things for directory/page entries */
const READ_WRITE_BIT: u32 = 1 << 1;
/* if set, page is user-accessible.  This means different things for directory
 * and table entries  */
const USER_SUPER_BIT: u32 = 1 << 2;
/* if set, write-through caching is enabled for that page/page table, otherwise,
 * write-back is used (usually left unset) */
/* if set in page table entry, virtual-to-physical mapping will not be
 * automatically flushed from TLB when writing %cr3. Used to prevent kernel
 * mappings from being flushed on context switches.  To use, %cr4 must be set */
const GLOBAL_BIT: u32 = 1 << 8;
/* when set, indicates that page table is in fact a 4MB page */
const PAGE_SIZE_BIT: u32 = 1 << 7;
// denotes whether a page is read-only because it's COW
const COW_BIT: u32 = 1 << 9;


/* hosts a page directory and its associated locking */
struct PD_t {
    PT_entries: **u32,
}

/*** Simple manipulation functions */
/* XXX: I am not totes thrilled with representing frames as *u8 -sully */
/* returns the page directory index from an address */
fn get_pd_entry(addr: *u8) -> u32 {
    (((addr as u32)&PD_MASK) >> PAGE_DIR_OFFSET) as u32
}
/* returns the page table index from an address */
fn get_pt_entry(addr: *u8) -> u32 {
    (((addr as u32)&PT_MASK) >> PAGE_TABLE_OFFSET) as u32
}
/* returns flags from an address */
fn get_flags(addr: *u8) -> u32 {
    (addr as u32) & FLAG_MASK
}
/* returns whether an address has a particular flag */
fn has_flags(addr: *u8, flag: u32) -> bool {
    ((addr as u32) & flag) != 0
}
/* places the components of a logical address together appropriately */
fn make_entry(pd: u32, pt: u32, ofs: u32) -> *u8 {
    ((pd << PAGE_DIR_OFFSET) | (pt << PAGE_TABLE_OFFSET) | ofs) as *u8
}

/* page-aligns addr */
fn get_aligned_addr(addr: *u32) -> *u32 {
    ((addr as u32)&(PD_MASK | PT_MASK)) as *u32
}

fn is_addr_aligned(addr: *u8) -> bool {
    (addr == get_aligned_addr(addr as *u32) as *u8)
}

/*** VM Implementation ***/

/* Used in the main kernel function to turn on paging once a page directory is
 * loaded.  Paging should probably not be turned off at any point in the kernel */
fn enable_paging() {
    // set the Page Size Extension bit in cr4 to enable variable length pages
    let cr4: u32 = get_cr4();
    set_cr4(cr4 | (1 << 4));

    // set the paging bit in cr0 to enable paging
    // set write protect bit so we fault on read-only pages in the kernel
    let cr0: u32 = get_cr0();
    cr0|= ((1 << 31) | (1 << 16));
    set_cr0(cr0);
}

/* loads the virtual -> physical mappings from PD into %cr3 */
fn load_PD(PD: *PD_t) {
    set_cr3(PD->PT_entries as u32);
}

/* Given a valid logical address and a page directory, returns address of the physical
 * frame that maps to the address in PD. returns null if no frame is found.  */
fn log_to_phys(log_addr: *u8, PD: *PD_t) -> **u8 {
    assert(is_addr_aligned(log_addr));
    if (PD == null) {
        //PD = get_cr3() as *PD_t;
        printf0_("PD null\n");

        return null;
    }

    let pd_entry: u32 = get_pd_entry(log_addr);
    let pt_entry: u32 = get_pt_entry(log_addr);
    if (PD->PT_entries == null) {
        printf0_("entries null\n");

        return null;
    }
    let page_table: *u32 = get_aligned_addr(PD->PT_entries[pd_entry]);

    if (page_table == null) {
        return null;
    }

    /* if the entry is a 4MB page, return at the top level */
    if (has_flags(PD->PT_entries[pd_entry] as *u8, PAGE_SIZE_BIT)) {

        return &(PD->PT_entries[pd_entry]) as **u8;
    }

    (&(get_aligned_addr((PD->PT_entries[pd_entry]))[pt_entry] )) as **u8
}

fn set_flags(log_addr: *u8, PD: *PD_t, flags: u32) {
    let frame_addr = log_to_phys(log_addr, PD);
    let frame_no_flags = ((*frame_addr as u32) & (~FLAG_MASK)); // clear out flag bits
    frame_no_flags|=flags;
    *frame_addr = frame_no_flags as *u8;
}

fn get_frame_ref(frame: *u8) -> u32 {
    let frame_num: u32 = (frame as u32)/PAGE_SIZE;
    assert(frame_num > 0 && frame_num < MAX_FRAME_NUM);
    (frame_table[frame_num])
}

fn dec_frame_ref(frame: *u8) -> u32 {
    let frame_num: u32 = (frame as u32)/PAGE_SIZE;
    assert(frame_num > 0 && frame_num < MAX_FRAME_NUM);

    let ref_count: *u32 = &(frame_table[frame_num]);
    assert(*ref_count > 0);
    let old_ref = atomic_add(ref_count, -1);
    if (old_ref == 1) {
        // add frame to free list (size-dependent)
    }
    old_ref
}

fn inc_frame_ref(frame: *u8) {
    let frame_num: u32 = (frame as u32)/PAGE_SIZE;
    assert(frame_num > 0 && frame_num < MAX_FRAME_NUM);
    let ref_count: *u32 = &(frame_table[frame_num]);
    atomic_add(ref_count, 1);
    assert(*ref_count > 0);
}

// we assume VM locking has been taken care of
fn reuse_frame(log_addr: *u8, PD_source: *PD_t, PD_dest: *PD_t) -> i32 {

    if (!(log_addr >= USER_MEM_START)) {
        printf1_("INCORRECT ADDR %x\n", log_addr as u32);
        assert(false);
    }

    let frame_addr: **u8 = log_to_phys(log_addr, PD_source);
    if (frame_addr == null) {
        return 0;
    }
    let frame = *frame_addr;
    if (frame == null) {
        return 0;
    }
    let size: u32 = KB;
    if (has_flags(frame, PAGE_SIZE_BIT) == 1) {
        size = MB;
    }    
    let old_frame_ref =  get_frame_ref(frame);
    assert(old_frame_ref > 0);

    let ret = set_page(frame, log_addr, PD_dest, get_flags(log_addr), size); 

    // No extraneous increments, no sir 
    assert(get_frame_ref(frame) == old_frame_ref + 1);
    // if setting the page failed, return with failure
    if (ret < 0) {
        return ret;
    }
    // both frames are set to read-only
    set_flags(log_addr, PD_source, PRESENT_BIT | USER_SUPER_BIT | COW_BIT); 
    set_flags(log_addr, PD_dest, PRESENT_BIT | USER_SUPER_BIT | COW_BIT); 

    0
}

fn free_page(log_addr: *u8, PD: *PD_t) {
    assert(log_addr >= USER_MEM_START);
    assert(is_addr_aligned(log_addr));

    let page_entry: **u8 = log_to_phys(log_addr, PD);
    if (page_entry == null) {
        return ();
    }

    /* if we are the only PD holding this page, free it */
    /* there is a race where we can overcommit by a frame.  We'll see how much I
     * care  */
    let old_ref = dec_frame_ref(*page_entry);

    *page_entry = null;
    // TODO invalidate TLB

}

/*
 * frame: the physical frame being set
 * log_addr: the logical address getting backed by the frame
 * requires that page being set does not already exist in the directory 
 **/
fn set_page(frame: *u8, log_addr: *u8, PD: *PD_t, flags: u32, size: u32) -> i32 {
    assert(is_addr_aligned(log_addr));
    // TODO when we have sized arrays, we can make this work
    if (PD == null) {
        printf0_("PD null\n");
        // PD = get_cr3() as *PD_t;
        assert(false);
    }
    assert(PD != null);
    assert(frame < USER_MEM_START);
    assert(PD->PT_entries != null);

    let pd_entry: u32 = get_pd_entry(log_addr);
    let pt_entry: u32 = get_pt_entry(log_addr);

    let page_table_addr: **u32 = &(PD->PT_entries[pd_entry]);

    /* if the page we're allocating is 4KB */
    if (size == KB) {
        if (*page_table_addr == null) {
            *page_table_addr = frame_alloc(0) as *u32;
            if (*page_table_addr == null) {
                return -1;
            }
            mos_memset((*page_table_addr as *u32), 0, PAGE_SIZE);
            *page_table_addr = ((*page_table_addr as u32) | READ_WRITE_BIT |
                    PRESENT_BIT | USER_SUPER_BIT) as *u32;
        }
        assert(*page_table_addr != null);
        let page_table: *u32 = get_aligned_addr(*page_table_addr);
        /* we don't want to overwrite shit */
        assert(page_table[pt_entry] == null);

        /* set the frame entry and flags */
        page_table[pt_entry] = (frame as u32) | flags | PRESENT_BIT;

    } else {
        assert(size == MB);
        *page_table_addr = frame as *u32;

        *page_table_addr = ((*page_table_addr as u32) | flags |
                PRESENT_BIT  | PAGE_SIZE_BIT) as *u32;
    }

    /* I think we can confidently place the increment here, trusting that the
     * frame we've been handed isn't shared with anyone else */
    if (log_addr >= USER_MEM_START) {
        inc_frame_ref(frame);
    }
    if (get_aligned_addr((*(log_to_phys(log_addr, PD) as *u32)) as *u32) !=
            get_aligned_addr(frame as *u32) as *u32)
    {
        printf2_("%x %x\n", *(log_to_phys(log_addr, PD) as *u32) as u32, frame as
                u32);
        //  assert(false);
    }
    0
}

fn new_user_page(log_addr: *u8, PD: *PD_t, flags: u32, size: u32) -> i32 {
    assert(log_addr >= USER_MEM_START);
    assert(is_addr_aligned(log_addr));
    let new_frame: *u8 = frame_alloc(0) as *u8;

    if (new_frame == null) {
        return -1;
    }

    if (set_page(new_frame, log_addr, PD, (USER_SUPER_BIT | flags), size) < 1) {
        // TODO free newly-allocated frame
        return -1;
    }
    0
}

/* at some point, have all PDs reference the same set of direct-mapped kernel
 * pages
 */
fn kernel_direct_map(log_addr: *u8, PD: *PD_t, flags: u32, size: u32) -> i32 {
    assert(log_addr < USER_MEM_START);
    assert(is_addr_aligned(log_addr));
    let success = set_page(log_addr, log_addr, PD, flags, size);
    success
}

/* Ideally, we page-align the PD allocation and have a static array of some
 * sort.  Until the compiler allows this, though, our init will have a page
 * table allocation step.  Sorry.  */
fn PD_init(PD: *PD_t) -> i32 {

    assert(PD != null);
    PD->PT_entries = frame_alloc(0) as **u32; 
    if (PD->PT_entries == null) {
        return -1;
    }
    assert(is_addr_aligned(PD->PT_entries as *u8));
    let i: u32;
    for (i = 0; i < PTS_PER_DIR; i+=1) {
        assert(PD->PT_entries != null);
        PD->PT_entries[i] = null;
    }
    let kernel_addr: *u8 = (PAGE_SIZE as *u8);
    /* leave the 0 page unmapped */
    /* if the init PD is not available or this is the init PD, map own copy of
     * kernel memory*/
    if (init_PD == null || PD == init_PD) {
        while (kernel_addr < USER_MEM_START) {
            if (get_pd_entry(kernel_addr) == 0) {
                if (kernel_direct_map(kernel_addr, PD, READ_WRITE_BIT, KB) < 0) {
                    // we don't free on error because we only hit this case in
                    // the init stage
                    return -1;
                }
                kernel_addr+=PAGE_SIZE;
            } else {
                if (kernel_direct_map(kernel_addr, PD, READ_WRITE_BIT, MB) < 0) {
                    // we don't free on error because we only hit this case in
                    // the init stage
                    return -1;
                }
                kernel_addr += (4 * MB);
            }
        }
    } else { // otherwise, just copy the page tables from the init_PD
        while (kernel_addr < USER_MEM_START) {
            PD->PT_entries[get_pd_entry(kernel_addr)] =
                init_PD->PT_entries[get_pd_entry(kernel_addr)];
            kernel_addr += (4 * MB);
        }
    }
    0
}
