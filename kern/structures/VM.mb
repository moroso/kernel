/**
 * VM.mb: contains implementations of kernel virtual memory operations
 * A static region of the address space represents kernel memory, and is
 * direct-mapped into a PD.  The rest is user memory, and is mapped into a PD at
 * will.  Pages can be mapped in as a 4KB page table entry (2-level), or a 4MB page directory
 * entry (1-level).  There are plans for a locking scheme, but one does not
 * currently exist.
 * XXX TODO TLB invalidation, freeing pages, single entry for kernel pages
 * across all PDs 
 *
 * Author: Amanda M. Watson
*/

const PAGES_PER_PT: u32 = 1024;
const PTS_PER_DIR: u32 = 1024;

/* used to distinguish between 4KB and 4MB page sizes */
// size of a kilobyte
const KB: u32 = 1024;
// size of a megabyte
#define MB (KB * 1024)

// offset into address where page directory entry is located
const PAGE_DIR_OFFSET: u32 = 22;
// offset into address where page table entry is located
const PAGE_TABLE_OFFSET: u32 = 12;
/* masks for interpreting portions of a logical address */
const PD_MASK: u32 = 0xFFC00000;
const PT_MASK: u32 = 0x003FF000;
#define FLAG_MASK (~((PD_MASK) | (PT_MASK)))
/* returns the page directory index from an address */
#define GET_PD_ENTRY(addr) ((((addr as u32)&PD_MASK) >> PAGE_DIR_OFFSET) as u32)
/* returns the page table index from an address */
#define GET_PT_ENTRY(addr) ((((addr as u32)&PT_MASK) >> PAGE_TABLE_OFFSET) as u32)
/* returns flags from an address */
#define GET_FLAGS(addr) (((u32)addr)&(FLAG_MASK))
/* returns whether an address has a particular flag */
#define HAS_FLAG(addr, flag) ((u32)addr&flag)
/* places the components of a logial address together appropriately */
#define MAKE_ENTRY(pd, pt, flags) (((pd) << PAGE_DIR_OFFSET) | \
        ((pt) << PAGE_TABLE_OFFSET) | (flags))

/* in both tables and entries.  if 0, means that all attempts
 * at access should result in a page fault */
const PRESENT_BIT: u32 = 1;
/* if set, page is writable.
 * means different things for directory/page entries */
#define READ_WRITE_BIT (1 << 1)
/* if set, page is user-accessible.  This means different things for directory
 * and table entries  */
#define USER_SUPER_BIT (1 << 2)
/* if set, write-through caching is enabled for that page/page table, otherwise,
 * write-back is used (usually left unset) */
/* if set in page table entry, virtual-to-physical mapping will not be
 * automatically flushed from TLB when writing %cr3. Used to prevent kernel
 * mappings from being flushed on context switches.  To use, %cr4 must be set */
#define GLOBAL_BIT (1 << 8)
/* when set, indicates that page table is in fact a 4MB page */
#define PAGE_SIZE_BIT (1 << 7)

/* page-aligns addr */
#define GET_ALIGNED_ADDR(addr) (((addr as u32)&(PD_MASK | PT_MASK)) as *u32)

/* hosts a page directory and its associated locking */
struct PD_t {
    PT_entries: **u32,
}


/*** VM Implementation ***/

/* Used in the main kernel function to turn on paging once a page directory is
 * loaded.  Paging should probably not be turned off at any point in the kernel */
fn enable_paging() {
    // set the Page Size Extension bit in cr4 to enable variable length pages
    let cr4: u32 = get_cr4();
    set_cr4(cr4 | (1 << 4));
    
    // set the paging bit in cr0 to enable paging
    let cr0: u32 = get_cr0();
    cr0|= (1 << 31);
    set_cr0(cr0);
}

/* loads the virtual -> physical mappings from PD into %cr3 */
fn load_PD(PD: *PD_t) {
    set_cr3(PD->PT_entries as u32);
}

/* Given a valid logical address and a page directory, returns the physical
 * frame that maps to the address in PD. returns null if no frame is found.  */
fn log_to_phys(log_addr: *u8, PD: *PD_t) -> *u8 {

    let pd_entry: u32 = GET_PD_ENTRY(log_addr);
    let pt_entry: u32 = GET_PT_ENTRY(log_addr);     

    if (PD->PT_entries == null) {
        return null;
    }    
    let page_table: *u32 = GET_ALIGNED_ADDR(PD->PT_entries[pd_entry]);
    if (page_table == null) {
        return null;
    }
    page_table[pt_entry] as *u8
}

/* 
 * frame: the physical frame being set
 * log_addr: the logical address getting backed by the frame
 **/
fn set_page (frame: *u8, log_addr: *u8, PD: *PD_t, flags: u32, size: u32) -> i32 {
    if (PD == null) {
        PD = get_cr3() as *PD_t;
    }
    assert(PD != null); 
    assert(frame < USER_MEM_START);     
    assert(PD->PT_entries != null);    

    let pd_entry: u32 = GET_PD_ENTRY(log_addr);
    let pt_entry: u32 = GET_PT_ENTRY(log_addr);                  

    let page_table_addr: **u32 = &(PD->PT_entries[pd_entry]);
    /* if the page we're allocating is 4KB */
    if (size == KB) {
        if (*page_table_addr == null) {
            *page_table_addr = frame_alloc(0) as *u32;     

            if (*page_table_addr == null) {
                return -1;
            }
            mos_memset((*page_table_addr as *u32), 0, PAGE_SIZE);
            *page_table_addr = ((*page_table_addr as u32) | READ_WRITE_BIT |
                    PRESENT_BIT | USER_SUPER_BIT) as *u32; 
        }
        assert(*page_table_addr != null);
        let page_table: *u32 = GET_ALIGNED_ADDR(*page_table_addr);
        /* we don't want to overwrite shit */
        assert(page_table[pt_entry] == null);
        /* set the frame entry and flags */

        page_table[pt_entry] = (frame as u32) | flags | PRESENT_BIT;   

    } else {
        assert(size == MB);
        *page_table_addr = frame as *u32;

        *page_table_addr = ((*page_table_addr as u32) | flags | 
                PRESENT_BIT  | PAGE_SIZE_BIT) as *u32; 

    }  
    0
}

fn new_user_page(log_addr: *u8, PD: *PD_t, flags: u32, size: u32) -> i32 {
    /* TODO let's do some checks to make sure we don't already have a frame
     * there */
    assert(log_addr >= USER_MEM_START);
    let new_frame: *u8 = frame_alloc(0) as *u8;

    if (new_frame == null) {
        return -1;
    }
    set_page(new_frame, log_addr, PD, (USER_SUPER_BIT | flags), size)
}

/* at some point, have all PDs reference the same set of direct-mapped kernel
 * pages 
 */
fn kernel_direct_map(log_addr: *u8, PD: *PD_t, flags: u32, size: u32) -> i32 {
    assert(log_addr < USER_MEM_START); 
    let success = set_page(log_addr, log_addr, PD, flags, size);   
    success
}

/* Ideally, we page-align the PD allocation and have a static array of some
 * sort.  Until the compiler allows this, though, our init will have a page
 * table allocation step.  Sorry.  */
fn PD_init(PD: *PD_t) -> i32 {
    assert(PD != null);
    PD->PT_entries = frame_alloc(0) as **u32; // 4 = sizeof(u32) 
    let i: u32;
    for (i = 0; i < PTS_PER_DIR; i+=1) {
        PD->PT_entries[i] = null;
    } 
    let kernel_addr: *u8 = (PAGE_SIZE as *u8);
    /* leave the 0 page unmapped */
    while (kernel_addr < USER_MEM_START) {
        if (GET_PD_ENTRY(kernel_addr) == 0) {
            if (kernel_direct_map(kernel_addr, PD, READ_WRITE_BIT, KB) < 0) {
                // TODO free prev alloc'd page tables on error 
                return -1;
            } 
            kernel_addr+=PAGE_SIZE;
        } else {
            if (kernel_direct_map(kernel_addr, PD, READ_WRITE_BIT, MB) < 0) {
                // TODO free prev alloc'd page tables on error 
                return -1;
            } 
            kernel_addr+= (4 * MB);
        }               
    }
    0
}

