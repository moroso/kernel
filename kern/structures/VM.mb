/**
 * VM.mb: contains implementations of kernel virtual memory operations
 * A static region of the address space represents kernel memory, and is
 * direct-mapped into a PD.  The rest is user memory, and is mapped into a PD at
 * will.  Pages can be mapped in as a 4KB page table entry (2-level), or a 4MB page directory
 * entry (1-level).  There are plans for an elaborate locking scheme, but one does not
 * currently exist.
 * Author: Amanda M. Watson
 */

use consts::*;
use locks::lol_atomics::*;
use utils::buddy_alloc::*;
use utils::stdlib::*;
use list::*;

// XXX: these shouldn't be here!
extern fn invalidate_tlb(frame: *u8);
extern fn get_cr0() -> u32;
extern fn get_cr2() -> u32;
extern fn get_cr3() -> u32;
extern fn get_cr4() -> u32;
extern fn set_cr0(val: u32);
extern fn set_cr3(val: u32);
extern fn set_cr4(val: u32);

const PAGES_PER_PT: u32 = 1024;
const PTS_PER_DIR: u32 = 1024;

/* initial page directory. contains only kernel memory */
static init_PD: *PD_t = null;
/* contains ref counts for each frame */
static frame_table: *u32 = null;

/* used to distinguish between 4KB and 4MB page sizes */
// size of a kilobyte
const KB: u32 = 1024;
// size of a megabyte
const MB: u32 = KB * 1024;

// offset into address where page directory entry is located
const PAGE_DIR_OFFSET: u32 = 22;
// offset into address where page table entry is located
const PAGE_TABLE_OFFSET: u32 = 12;
/* masks for interpreting portions of a logical address */
const PD_MASK: u32 = 0xFFC00000;
const PT_MASK: u32 = 0x003FF000;
const FLAG_MASK: u32 = ~((PD_MASK) | (PT_MASK));

/* in both tables and entries.  if 0, means that all attempts
 * at access should result in a page fault */
const PRESENT_BIT: u32 = 1;
/* if set, page is writable.
 * means different things for directory/page entries */
const READ_WRITE_BIT: u32 = 1 << 1;
/* if set, page is user-accessible.  This means different things for directory
 * and table entries  */
const USER_SUPER_BIT: u32 = 1 << 2;
/* if set, write-through caching is enabled for that page/page table, otherwise,
 * write-back is used (usually left unset) */
/* if set in page table entry, virtual-to-physical mapping will not be
 * automatically flushed from TLB when writing %cr3. Used to prevent kernel
 * mappings from being flushed on context switches.  To use, %cr4 must be set */
const GLOBAL_BIT: u32 = 1 << 8;
/* when set, indicates that page table is in fact a 4MB page */
const PAGE_SIZE_BIT: u32 = 1 << 7;
// denotes whether a page is read-only because it's COW
const COW_BIT: u32 = 1 << 9;


/* hosts a page directory and its associated locking */
struct PD_t {
    PT_entries: **u32,
}

/*** Simple manipulation functions */
/* XXX: I am not totes thrilled with representing frames as *u8 -sully */
/* returns the page directory index from an address */
fn get_pd_entry(addr: *u8) -> u32 {
    (((addr as u32)&PD_MASK) >> PAGE_DIR_OFFSET) as u32
}
/* returns the page table index from an address */
fn get_pt_entry(addr: *u8) -> u32 {
    (((addr as u32)&PT_MASK) >> PAGE_TABLE_OFFSET) as u32
}
/* returns flags from an address */
fn get_flags(addr: *u8) -> u32 {
    (addr as u32) & FLAG_MASK
}
/* returns whether an address has a particular flag */
fn has_flags(addr: *u8, flag: u32) -> bool {
    ((addr as u32) & flag) != 0
}
/* places the components of a logical address together appropriately */
fn make_entry(pd: u32, pt: u32, ofs: u32) -> *u8 {
    ((pd << PAGE_DIR_OFFSET) | (pt << PAGE_TABLE_OFFSET) | ofs) as *u8
}

/* page-aligns addr */
fn get_aligned_addr(addr: *u32) -> *u32 {
    ((addr as u32)&(PD_MASK | PT_MASK)) as *u32
}

fn is_addr_aligned(addr: *u8) -> bool {
    (addr == get_aligned_addr(addr as *u32) as *u8)
}

fn is_user_addr<T>(addr: *T) -> bool {
    addr as u32 >= USER_MEM_START
}
fn is_kernel_addr<T>(addr: *T) -> bool {
    !is_user_addr(addr)
}

/*** VM Implementation ***/

/* Used in the main kernel function to turn on paging once a page directory is
 * loaded.  Paging should probably not be turned off at any point in the kernel */
fn enable_paging() {
    // set the Page Size Extension bit in cr4 to enable variable length pages
    let cr4: u32 = get_cr4();
    set_cr4(cr4 | (1 << 4));

    // set the paging bit in cr0 to enable paging
    // set write protect bit so we fault on read-only pages in the kernel
    let cr0: u32 = get_cr0();
    cr0|= ((1 << 31) | (1 << 16));
    set_cr0(cr0);
}

/* loads the virtual -> physical mappings from PD into %cr3 */
fn load_PD(PD: *PD_t) {
    set_cr3(PD->PT_entries as u32);
}

/* Given a valid logical address and a page directory, returns address of the physical
 * frame that maps to the address in PD. returns null if no frame is found.  */
fn log_to_phys(log_addr: *u8, PD: *PD_t) -> **u8 {
    assert!(PD != null);

    let pd_entry: u32 = get_pd_entry(log_addr);
    let pt_entry: u32 = get_pt_entry(log_addr);
    if (PD->PT_entries == null) {
        printf0_("entries null\n");

        return null;
    }
    let page_table: *u32 = get_aligned_addr(PD->PT_entries[pd_entry]);

    if (page_table == null) {
        return null;
    }

    /* if the entry is a 4MB page, return at the top level */
    if (has_flags(PD->PT_entries[pd_entry] as *u8, PAGE_SIZE_BIT)) {

        return &(PD->PT_entries[pd_entry]) as **u8;
    }

    (&(get_aligned_addr((PD->PT_entries[pd_entry]))[pt_entry] )) as **u8
}

fn get_frame_size(log_addr: *u8, PD: *PD_t) -> u32 {

    assert!(is_addr_aligned(log_addr));
    assert!(PD != null);

    let pd_entry: u32 = get_pd_entry(log_addr);

    if (PD->PT_entries == null) {
        printf0_("entries null\n");

        return -1;
    }
    let page_table: *u32 = get_aligned_addr(PD->PT_entries[pd_entry]);

    if (page_table == null) {
        return -1;
    }

    if (has_flags(PD->PT_entries[pd_entry] as *u8, PAGE_SIZE_BIT)) {

        return MB;
    }
    KB

}

fn set_flags(log_addr: *u8, PD: *PD_t, flags: u32) {
    let frame_addr = log_to_phys(log_addr, PD);
    let frame_no_flags = ((*frame_addr as u32) & (~FLAG_MASK)); // clear out flag bits
    frame_no_flags|=flags;
    *frame_addr = frame_no_flags as *u8;
}

fn get_frame_ref(frame: *u8) -> u32 {
    let frame_num: u32 = (frame as u32)/PAGE_SIZE;
    assert!(frame_num > 0 && frame_num < MAX_FRAME_NUM);
    (frame_table[frame_num])
}

fn dec_frame_ref(frame: *u8, PD: *PD_t) -> u32 {
    let frame_num: u32 = (frame as u32)/PAGE_SIZE;
    assert!(frame_num > 0 && frame_num < MAX_FRAME_NUM);

    let ref_count: *u32 = &(frame_table[frame_num]);
    assert!(*ref_count > 0);
    let old_ref = atomic_add(ref_count, -1);
    if (old_ref == 1) {

        let size: u32 = get_frame_size(frame, PD);
        free_frame(frame, size);
    }
    old_ref
}

fn inc_frame_ref(frame: *u8) {
    let frame_num: u32 = (frame as u32)/PAGE_SIZE;
    assert!(frame_num > 0 && frame_num < MAX_FRAME_NUM);
    let ref_count: *u32 = &(frame_table[frame_num]);
    atomic_add(ref_count, 1);
    assert!(*ref_count > 0);
}

// we assume VM locking has been taken care of
fn reuse_frame(log_addr: *u8, PD_source: *PD_t, PD_dest: *PD_t) -> i32 {
    assert!(is_user_addr(log_addr));

    let frame_addr: **u8 = log_to_phys(log_addr, PD_source);
    if (frame_addr == null) {
        return 0;
    }
    let frame = *frame_addr;
    if (frame == null) {
        return 0;
    }
    let size: u32 = KB;
    if (has_flags(frame, PAGE_SIZE_BIT)) {
        size = MB;
    }
    let old_frame_ref =  get_frame_ref(frame);
    assert!(old_frame_ref > 0);

    let ret = set_page(frame, log_addr, PD_dest, get_flags(log_addr), size);

    // No extraneous increments, no sir
    assert!(get_frame_ref(frame) == old_frame_ref + 1);
    // if setting the page failed, return with failure
    if (ret < 0) {
        return ret;
    }
    // both frames are set to read-only
    set_flags(log_addr, PD_source, PRESENT_BIT | USER_SUPER_BIT | COW_BIT);
    set_flags(log_addr, PD_dest, PRESENT_BIT | USER_SUPER_BIT | COW_BIT);

    0
}

fn free_page(log_addr: *u8, PD: *PD_t) {
    assert!(is_user_addr(log_addr));
    assert!(is_addr_aligned(log_addr));

    let page_entry: **u8 = log_to_phys(log_addr, PD);
    if (page_entry == null) {
        return ();
    }

    let frame: *u8 = *page_entry;
    /* if we are the only PD holding this page, free it */
    /* there is a race in the handler where we can overcommit by a frame.  We'll see how much I
     * care  */
    dec_frame_ref(frame, PD);
    *page_entry = null;
    invalidate_tlb(frame);

}

/*
 * frame: the physical frame being set
 * log_addr: the logical address getting backed by the frame
 * requires that page being set does not already exist in the directory
 */
fn set_page(frame: *u8, log_addr: *u8, PD: *PD_t, flags: u32, size: u32) -> i32 {
    assert!(is_addr_aligned(log_addr));
    assert!(PD != null);
    assert!(PD != null);
    assert!(is_kernel_addr(frame));
    assert!(PD->PT_entries != null);

    let pd_entry: u32 = get_pd_entry(log_addr);
    let pt_entry: u32 = get_pt_entry(log_addr);

    let page_table_addr: **u32 = &(PD->PT_entries[pd_entry]);

    /* if the page we're allocating is 4KB */
    if (size == KB) {
        if (*page_table_addr == null) {
            *page_table_addr = frame_alloc() as *u32;
            if (*page_table_addr == null) {
                printf0_("out of memory\n");
                return -1;
            }
            mos_memset((*page_table_addr as *u32), 0, PAGE_SIZE);
            *page_table_addr = ((*page_table_addr as u32) | READ_WRITE_BIT |
                    PRESENT_BIT | USER_SUPER_BIT) as *u32;
        }
        assert!(*page_table_addr != null);
        let page_table: *u32 = get_aligned_addr(*page_table_addr);
        /* we don't want to overwrite shit */
        assert!(page_table[pt_entry] == 0);

        /* set the frame entry and flags */
        page_table[pt_entry] = (frame as u32) | flags | PRESENT_BIT;

    } else {
        assert!(size == MB);
        *page_table_addr = frame as *u32;

        *page_table_addr = ((*page_table_addr as u32) | flags |
                PRESENT_BIT  | PAGE_SIZE_BIT) as *u32;
    }

    /* I think we can confidently place the increment here, trusting that the
     * frame we've been handed isn't shared with anyone else */
    if (is_user_addr(log_addr)) {
        inc_frame_ref(frame);
    }
    if (get_aligned_addr((*(log_to_phys(log_addr, PD) as *u32)) as *u32) !=
        get_aligned_addr(frame as *u32) as *u32)
    {
        printf2_("page not actually set in set_page(): %x %x\n",
                 *(log_to_phys(log_addr, PD) as *u32) as u32, frame as u32);
        assert!(false);
    }
    0
}

fn get_frame(size: u32) -> *u8 {
    if (size == KB) {
        return frame_alloc() as *u8;
    }
    assert!(size == MB);
    frame_alloc_pow2(10) as *u8
}

fn free_frame(frame: *u8, size: u32) {

    if (size == KB) {
        frame_free(frame);
    } else {
        assert!(size == MB);
        frame_free_pow2(frame, 10);
    }
}

fn new_user_page(log_addr: *u8, PD: *PD_t, flags: u32, size: u32) -> i32 {
    assert!(is_user_addr(log_addr));
    assert!(is_addr_aligned(log_addr));
    let new_frame: *u8 = get_frame(KB);

    if (new_frame == null) {
        printf0_("out of memory\n");
        return -1;
    }

    if (set_page(new_frame, log_addr, PD, (USER_SUPER_BIT | flags), size) < 0) {
        // TODO free newly-allocated frame
        printf0_("other reason\n");
        return -1;
    }
    0
}

/* at some point, have all PDs reference the same set of direct-mapped kernel
 * pages
 */
fn kernel_direct_map(log_addr: *u8, PD: *PD_t, flags: u32, size: u32) -> i32 {
    assert!(is_kernel_addr(log_addr));
    assert!(is_addr_aligned(log_addr));
    let success = set_page(log_addr, log_addr, PD, flags, size);
    success
}

/* Ideally, we page-align the PD allocation and have a static array of some
 * sort.  Until the compiler allows this, though, our init will have a page
 * table allocation step.  Sorry.  */
fn PD_init(PD: *PD_t) -> i32 {
    assert!(PD != null);
    PD->PT_entries = frame_alloc() as **u32;
    if (PD->PT_entries == null) {
        return -1;
    }
    assert!(is_addr_aligned(PD->PT_entries as *u8));
    let i: u32;
    for (i = 0; i < PTS_PER_DIR; i+=1) {
        assert!(PD->PT_entries != null);
        PD->PT_entries[i] = null;
    }
    let kernel_addr: *u8 = (PAGE_SIZE as *u8);
    /* leave the 0 page unmapped */
    /* if the init PD is not available or this is the init PD, map own copy of
     * kernel memory*/
    if (init_PD == null || PD == init_PD) {
        while is_kernel_addr(kernel_addr) {
            if (get_pd_entry(kernel_addr) == 0) {
                if (kernel_direct_map(kernel_addr, PD, READ_WRITE_BIT, KB) < 0) {
                    // we don't free on error because we only hit this case in
                    // the init stage
                    return -1;
                }
                kernel_addr+=PAGE_SIZE;
            } else {
                if (kernel_direct_map(kernel_addr, PD, READ_WRITE_BIT, MB) < 0) {
                    // we don't free on error because we only hit this case in
                    // the init stage
                    return -1;
                }
                kernel_addr += (4 * MB);
            }
        }
    } else { // otherwise, just copy the page tables from the init_PD
        while is_kernel_addr(kernel_addr) {
            PD->PT_entries[get_pd_entry(kernel_addr)] =
                init_PD->PT_entries[get_pd_entry(kernel_addr)];
            kernel_addr += (4 * MB);
        }
    }
    0
}

/* Invoked by the page fault handler; it's assumed that, when cow_copy is called
 * on an address, checks have been made to ensure that it is, in fact, a COW
 * page in the PD.
 * given a address and a PD, ensures the PD has a writable, unshared copy of the
 * frame log_addr points to in the PD.  If the frame has no other reference, it
 * is simply made writable.  Otherwise, a new writable frame replaces it, and is
 * also made writable.  Assuming overcommits are not possible, should not be
 * allowed to fail.  */
fn cow_copy(log_addr: *u8, PD: *PD_t) {
    let frame_orig_addr: **u8 = log_to_phys(log_addr, PD);
    assert!(frame_orig_addr != null);
    let frame_orig: *u8 = *frame_orig_addr;

    // if we are the only one holding the page, let's make it writable again
    if (get_frame_ref(frame_orig) == 1) {
        *frame_orig_addr = ((frame_orig as u32) | READ_WRITE_BIT) as *u8;
        return ();
    }
    // otherwise, we make a new page and copy it over
    let frame_size = get_frame_size(log_addr, PD);
    let frame_new: *u8 = get_frame(frame_size);
    mos_memcpy(frame_new, log_addr, frame_size);
    free_page(log_addr, PD); // removes page and decs ref count
    let set_page_result = set_page(frame_new, log_addr, PD,
                                   USER_SUPER_BIT | READ_WRITE_BIT,
                                   frame_size);
    assert!(set_page_result >= 0); // this assert trips if we run out of frames,
    // but should not once we deal with overcommitting
}
