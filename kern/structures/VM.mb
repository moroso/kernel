/**
 * VM.mb: contains implementations of kernel virtual memory operations
 * A static region of the address space represents kernel memory, and is
 * direct-mapped into a PD.  The rest is user memory, and is mapped into a PD at
 * will.  Pages can be mapped in as a 4KB page table entry (2-level), or a 4MB page directory
 * entry (1-level).  There are plans for an elaborate locking scheme, but one does not
 * currently exist.
 * XXX TODO TLB invalidation, freeing pages
 * Author: Amanda M. Watson
*/

const PAGES_PER_PT: u32 = 1024;
const PTS_PER_DIR: u32 = 1024;

/* initial page directory. contains only kernel memory */
static init_PD: *PD_t = null;

/* used to distinguish between 4KB and 4MB page sizes */
// size of a kilobyte
const KB: u32 = 1024;
// size of a megabyte
const MB: u32 = KB * 1024;

// offset into address where page directory entry is located
const PAGE_DIR_OFFSET: u32 = 22;
// offset into address where page table entry is located
const PAGE_TABLE_OFFSET: u32 = 12;
/* masks for interpreting portions of a logical address */
const PD_MASK: u32 = 0xFFC00000;
const PT_MASK: u32 = 0x003FF000;
const FLAG_MASK: u32 = ~((PD_MASK) | (PT_MASK));

/* in both tables and entries.  if 0, means that all attempts
 * at access should result in a page fault */
const PRESENT_BIT: u32 = 1;
/* if set, page is writable.
 * means different things for directory/page entries */
const READ_WRITE_BIT: u32 = 1 << 1;
/* if set, page is user-accessible.  This means different things for directory
 * and table entries  */
const USER_SUPER_BIT: u32 = 1 << 2;
/* if set, write-through caching is enabled for that page/page table, otherwise,
 * write-back is used (usually left unset) */
/* if set in page table entry, virtual-to-physical mapping will not be
 * automatically flushed from TLB when writing %cr3. Used to prevent kernel
 * mappings from being flushed on context switches.  To use, %cr4 must be set */
const GLOBAL_BIT: u32 = 1 << 8;
/* when set, indicates that page table is in fact a 4MB page */
const PAGE_SIZE_BIT: u32 = 1 << 7;


/* hosts a page directory and its associated locking */
struct PD_t {
    PT_entries: **u32,
    ref_entries: **u32,
}

/*** Simple manipulation functions */
/* XXX: I am not totes thrilled with representing frames as *u8 -sully */
/* returns the page directory index from an address */
fn get_pd_entry(addr: *u8) -> u32 {
    (((addr as u32)&PD_MASK) >> PAGE_DIR_OFFSET) as u32
}
/* returns the page table index from an address */
fn get_pt_entry(addr: *u8) -> u32 {
    (((addr as u32)&PT_MASK) >> PAGE_TABLE_OFFSET) as u32
}
/* returns flags from an address */
fn get_flags(addr: *u8) -> u32 {
    (addr as u32) & FLAG_MASK
}
/* returns whether an address has a particular flag */
fn has_flags(addr: *u8, flag: u32) -> bool {
    ((addr as u32) & flag) != 0
}
/* places the components of a logical address together appropriately */
fn make_entry(pd: u32, pt: u32, ofs: u32) -> *u8 {
    ((pd << PAGE_DIR_OFFSET) | (pt << PAGE_TABLE_OFFSET) | ofs) as *u8
}

/* page-aligns addr */
fn get_aligned_addr(addr: *u32) -> *u32 {
    ((addr as u32)&(PD_MASK | PT_MASK)) as *u32
}


/*** VM Implementation ***/

/* Used in the main kernel function to turn on paging once a page directory is
 * loaded.  Paging should probably not be turned off at any point in the kernel */
fn enable_paging() {
    // set the Page Size Extension bit in cr4 to enable variable length pages
    let cr4: u32 = get_cr4();
    set_cr4(cr4 | (1 << 4));

    // set the paging bit in cr0 to enable paging
    let cr0: u32 = get_cr0();
    cr0|= (1 << 31);
    set_cr0(cr0);
}

/* loads the virtual -> physical mappings from PD into %cr3 */
fn load_PD(PD: *PD_t) {
    set_cr3(PD->PT_entries as u32);
}

/* Given a valid logical address and a page directory, returns the physical
 * frame that maps to the address in PD. returns null if no frame is found.  */
fn log_to_phys(log_addr: *u8, PD: *PD_t) -> *u8 {

    let pd_entry: u32 = get_pd_entry(log_addr);
    let pt_entry: u32 = get_pt_entry(log_addr);

    if (PD->PT_entries == null) {
        return null;
    }
    let page_table: *u32 = get_aligned_addr(PD->PT_entries[pd_entry]);
    if (page_table == null) {
        return null;
    }
    page_table[pt_entry] as *u8
}

/* given a logical address in a page directory, returns a pointer to its frame's ref
 * count */
fn get_ref_count(log_addr: *u8, PD: *PD_t) -> *u32 {
    if (PD == null) {
        PD = get_cr3() as *PD_t;
    }
    assert(PD != null);

    let pd_entry: u32 = get_pd_entry(log_addr);
    let pt_entry: u32 = get_pt_entry(log_addr);
    let page_table_addr: **u32 = &(PD->ref_entries[pd_entry]);
    if (*page_table_addr == null) {
        *page_table_addr = frame_alloc(0) as *u32;

        if (*page_table_addr == null) {
            return null;
        }
        mos_memset((*page_table_addr as *u32), 0, PAGE_SIZE);

    }
    assert(*page_table_addr != null);
    &((*page_table_addr)[pt_entry])
}

/*
 * frame: the physical frame being set
 * log_addr: the logical address getting backed by the frame
 * requires that page being set does not already exist in the directory (so we
 * don't fuck up ref counts)
 **/
fn set_page (frame: *u8, log_addr: *u8, PD: *PD_t, flags: u32, size: u32) -> i32 {
    if (PD == null) {
        PD = get_cr3() as *PD_t;
    }
    assert(PD != null);
    assert(frame < USER_MEM_START);
    assert(PD->PT_entries != null);

    let pd_entry: u32 = get_pd_entry(log_addr);
    let pt_entry: u32 = get_pt_entry(log_addr);
    let ref_count: *u32;
 
    /* if we're allocating a user page, create a ref count */
    if (log_addr >= USER_MEM_START) {
        // allocate and return ref count (will also return if ref count already exists)
        ref_count = get_ref_count(log_addr, PD);
        if (ref_count == null) {
            return -1;
        }
        /* ref should not indicate a frame already exists */
        assert(*ref_count == 0);
    }

    let page_table_addr: **u32 = &(PD->PT_entries[pd_entry]);
    /* if the page we're allocating is 4KB */
    if (size == KB) {
        if (*page_table_addr == null) {
            *page_table_addr = frame_alloc(0) as *u32;

            if (*page_table_addr == null) {
                return -1;
            }
            mos_memset((*page_table_addr as *u32), 0, PAGE_SIZE);
            *page_table_addr = ((*page_table_addr as u32) | READ_WRITE_BIT |
                    PRESENT_BIT | USER_SUPER_BIT) as *u32;
        }
        assert(*page_table_addr != null);
        let page_table: *u32 = get_aligned_addr(*page_table_addr);
        /* we don't want to overwrite shit */
        assert(page_table[pt_entry] == null);
        /* set the frame entry and flags */

        page_table[pt_entry] = (frame as u32) | flags | PRESENT_BIT;

    } else {
        assert(size == MB);
        *page_table_addr = frame as *u32;

        *page_table_addr = ((*page_table_addr as u32) | flags |
                PRESENT_BIT  | PAGE_SIZE_BIT) as *u32;

    }

    if (log_addr >= USER_MEM_START) {
        /* we shouldn't have to do this atomically, since any new page should not be
         * shared by others */
        atomic_add(ref_count, 1); 
        assert(*ref_count == 1);
    }

    0
}

fn new_user_page(log_addr: *u8, PD: *PD_t, flags: u32, size: u32) -> i32 {
    /* TODO let's do some checks to make sure we don't already have a frame
     * there */
    assert(log_addr >= USER_MEM_START);
    let new_frame: *u8 = frame_alloc(0) as *u8;

    if (new_frame == null) {
        return -1;
    }
    set_page(new_frame, log_addr, PD, (USER_SUPER_BIT | flags), size)
}

/* at some point, have all PDs reference the same set of direct-mapped kernel
 * pages
 */
fn kernel_direct_map(log_addr: *u8, PD: *PD_t, flags: u32, size: u32) -> i32 {
    assert(log_addr < USER_MEM_START);
    let success = set_page(log_addr, log_addr, PD, flags, size);
    success
}

/* Ideally, we page-align the PD allocation and have a static array of some
 * sort.  Until the compiler allows this, though, our init will have a page
 * table allocation step.  Sorry.  */
fn PD_init(PD: *PD_t) -> i32 {
    assert(PD != null);
    PD->PT_entries = frame_alloc(0) as **u32; 
    PD->ref_entries = frame_alloc(0) as **u32;
    let i: u32;
    for (i = 0; i < PTS_PER_DIR; i+=1) {
        PD->PT_entries[i] = null;
    }
    let kernel_addr: *u8 = (PAGE_SIZE as *u8);
    /* leave the 0 page unmapped */
    /* if the init PD is not available or this is the init PD, map own copy of
     * kernel memory*/
    if (init_PD == null || PD == init_PD) {
        while (kernel_addr < USER_MEM_START) {
            if (get_pd_entry(kernel_addr) == 0) {
                if (kernel_direct_map(kernel_addr, PD, READ_WRITE_BIT, KB) < 0) {
                    // TODO free prev alloc'd page tables on error
                    return -1;
                }
                kernel_addr+=PAGE_SIZE;
            } else {
                if (kernel_direct_map(kernel_addr, PD, READ_WRITE_BIT, MB) < 0) {
                    // TODO free prev alloc'd page tables on error
                    return -1;
                }
                kernel_addr += (4 * MB);
            }
        }
    } else { // otherwise, just copy the page tables from the init_PD
        while (kernel_addr < USER_MEM_START) {
            PD->PT_entries[get_pd_entry(kernel_addr)] =
                init_PD->PT_entries[get_pd_entry(kernel_addr)];
            kernel_addr += (4 * MB);
        }
    }
    0
}
