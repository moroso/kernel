/**
 * VM.mb: contains implementations of kernel virtual memory operations
 * A static region of the address space represents kernel memory, and is
 * direct-mapped into a PD.  The rest is user memory, and is mapped into a PD at
 * will.  Pages can be mapped in as a 4KB page table entry (2-level), or a 4MB page directory
 * entry (1-level).  There are plans for a locking scheme, but one does not
 * currently exist.
 * XXX TODO TLB invalidation, freeing pages, single entry for kernel pages
 * across all PDs 
 *
 * Author: Amanda M. Watson
*/
#include "VM.mh"

/* Used in the main kernel function to turn on paging once a page directory is
 * loaded.  Paging should probably not be turned off at any point in the kernel */
fn enable_paging() {
    // set the Page Size Extension bit in cr4 to enable variable length pages
    let cr4: u32 = get_cr4();
    set_cr4(cr4 | (1 << 4));
    
    // set the paging bit in cr0 to enable paging
    let cr0: u32 = get_cr0();
    cr0|= (1 << 31);
    set_cr0(cr0);
}

/* loads the virtual -> physical mappings from PD into %cr3*/
fn load_PD(PD: *PD_t) {
    set_cr3(PD->PT_entries as u32);
}

/* Given a valid logical address and a page directory, returns the physical
 * frame that maps to the address in PD. returns null if no frame is found.  */
fn log_to_phys(log_addr: *u8, PD: *PD_t) -> *u8 {

    let pd_entry: u32 = GET_PD_ENTRY(log_addr);
    let pt_entry: u32 = GET_PT_ENTRY(log_addr);     

    if (PD->PT_entries == null) {
        return null;
    }    
    let page_table: *u32 = GET_ALIGNED_ADDR(PD->PT_entries[pd_entry]);
    if (page_table == null) {
        return null;
    }
    page_table[pt_entry] as *u8
}

/* 
 * frame: the physical frame being set
 * log_addr: the logical address getting backed by the frame
 **/
fn set_page (frame: *u8, log_addr: *u8, PD: *PD_t, flags: u32, size: u32) -> i32 {
    if (PD == null) {
        PD = get_cr3() as *PD_t;
    }
    assert(PD != null); 
    assert(frame < USER_MEM_START);     
    assert(PD->PT_entries != null);    

    let pd_entry: u32 = GET_PD_ENTRY(log_addr);
    let pt_entry: u32 = GET_PT_ENTRY(log_addr);                  

    let page_table_addr: **u32 = &(PD->PT_entries[pd_entry]);
    /* if the page we're allocating is 4KB */
    if (size == KB) {
        if (*page_table_addr == null) {
            *page_table_addr = frame_alloc(0) as *u32;     

            if (*page_table_addr == null) {
                return -1;
            }

            *page_table_addr = ((*page_table_addr as u32) | READ_WRITE_BIT |
                    PRESENT_BIT | USER_SUPER_BIT) as *u32; 
        }
        assert(*page_table_addr != null);
        let page_table: *u32 = GET_ALIGNED_ADDR(*page_table_addr);
        /* we don't want to overwrite shit */
        assert(page_table[pt_entry] == null);
        /* set the frame entry and flags */
        page_table[pt_entry] = (frame as u32) | flags | PRESENT_BIT;   

    } else {
        assert(size == MB);
        *page_table_addr = frame as *u32;
        *page_table_addr = ((*page_table_addr as u32) | flags | 
                PRESENT_BIT  | PAGE_SIZE_BIT) as *u32; 

    }  
    0
}

fn new_user_page(log_addr: *u8, PD: *PD_t, flags: u32, size: u32) -> i32 {
    /* TODO let's do some checks to make sure we don't already have a frame
     * there */
    assert(log_addr >= USER_MEM_START);
    let new_frame: *u8 = frame_alloc(0) as *u8;

    if (new_frame == null) {
        return -1;
    }
    set_page(new_frame, log_addr, PD, (USER_SUPER_BIT | flags), size)
}

/* at some point, have all PDs reference the same set of direct-mapped kernel
 * pages 
*/
fn kernel_direct_map(log_addr: *u8, PD: *PD_t, flags: u32, size: u32) -> i32 {
    assert(log_addr < USER_MEM_START); 
    let success = set_page(log_addr, log_addr, PD, flags, size);   
    success
}

/* Ideally, we page-align the PD allocation and have a static array of some
 * sort.  Until the compiler allows this, though, our init will have a page
 * table allocation step.  Sorry.  */
fn PD_init(PD: *PD_t) -> i32 {
    assert(PD != null);
    PD->PT_entries = frame_alloc(0) as **u32; // 4 = sizeof(u32) 
    let i: u32;
    for (i = 0; i < PTS_PER_DIR; i+=1) {
        PD->PT_entries[i] = null;
    } 
    let kernel_addr: *u8 = (PAGE_SIZE as *u8);
    /* leave the 0 page unmapped */
    while (kernel_addr < USER_MEM_START) {
        if (GET_PD_ENTRY(kernel_addr) == 0) {
            if (kernel_direct_map(kernel_addr, PD, READ_WRITE_BIT, KB) < 0) {
                // TODO free prev alloc'd page tables on error 
                return -1;
            } 
            kernel_addr+=PAGE_SIZE;
        } else {
            if (kernel_direct_map(kernel_addr, PD, READ_WRITE_BIT, MB) < 0) {
                // TODO free prev alloc'd page tables on error 
                return -1;
            } 
            kernel_addr+= (4 * MB);
        }               
    }
    0
}

