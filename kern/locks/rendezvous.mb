/* locks/rendezvous.mb: implementation of the rendezvous synchronization
 * primitive
 * 
 * It should be noted that "rendezvous" is something of a misnomer here. In our
 * implementation, there are two types of threads: a send thread and a receive
 * thread.  a rend struct has a send list and a receive list, one for each type
 * of thread.  And so, when a thread invokes rend_wait, it checks to see if a
 * thread is already in the list of the other type; if it is, the invoking
 * thread pops it off the list and returns with it.  Otherwise, it waits on its
 * own list for a thread of the other type to call rend_wait.
 * 
 * in each list, a thread is stored inside a rend_node, which contains
 * information about the transfer, including a message.  when rend_wait returns
 * a rend_node, this means the invoking thread is responsible for a) the message
 * transfer and b) waking up the tcb inside the rend_node.  
 *
 * A thread is reawoken by calling rend_signal (which should be called after the
 * message transfer) on the given rend_node.  
 * 
 * rend_nodes also have timeouts.  If the node times out while still on the
 * list, it gets removed and signaled with an error.  
 *
 * there are several sending options that can specify whether sender or receiver
 * performs the transfer.
 *
 * The justification for the weirdness was the notion that often, messages will
 * be sent without expecting a reply.  In these cases, it is faster to avoid the
 * complicated dance of waking up on thread while putting your own to sleep so
 * that the other thread can send you a message (as we would see with
 * cond_vars), and instead have the transfer performed by whichever thread is
 * not currently blocked on receipt.  
 *
 * We have a dubious design decision: on-the-fly dynamic memory
 * allocation for asynchronous messages (it is perhaps more tasteful to assume
 * the invoker has dynamically allocated their message)
 *
 * Author: Amanda M. Watson
*/

use locks::mutex::*;
use structures::tcb::*;
use utils::list::*;
use structures::schedule::*;
use consts::*;
use utils::slab_alloc::*;
use utils::stdlib::*;
use arch::entry::get_ticks;
use shared::ipc_defs::*;

/* sync options */
const SEND_SYNC_REPLY: u32 = 0;
const SEND_SYNC: u32 = 0;
const SEND_ASYNC: u32 = 2;
const SEND_SYNC_NOREPLY: u32 = 3;
const RECV: u32 = 1;

/* error options */
const REND_ERR_MEM: i32 = -1;
const REND_ERR_TIMEOUT: i32 = -5;
const REND_ERR_DEAD: i32 = -3;
const REND_ERR_REPLAY: i32 = -4;

/* The rendezvous synchronization primitive */
struct rend {
    send_head: list_head,
    recv_head: list_head,
    rend_active: bool,
}

/* represents a tcb passing a message through the rend.  allocated internally
 * but returned to the invoking thread so that they can process the message and
 * signal the blocked tcb */
struct rend_node {
    message: *u32,
    message_len: u32,
    tcb: *tcb,
    timeout: u32,
    can_timeout: bool,
    timeout_link: list_node,
    rend_link: list_node,
    option: u32,
    err: u32,
}

/* initializes a rend */
fn rend_init(rend: *rend) {
    list_init_head(&rend->recv_head);
    list_init_head(&rend->send_head);
    rend->rend_active = true;
}

/*** structures manipulating active rend_nodes with a timeout time.  inserted
 * during rend_wait and sorted by timeout; removed either when signaled, or when
 * their timeout expires (removal in timer handler) ***/
static rend_timeout_list: list_head = LIST_HEAD_INIT!(rend_timeout_list);

/* inserts a rend node into timeout queue, sorted by timeout time.  A timeout of
 * -1 denotes that rend_node cannot timeout, and so will not be added  */
fn rend_timeout_insert(rend_insert: *rend_node) {
    let enable = cond_preempt_disable();
    if (rend_insert->timeout == -1) {
        cond_preempt_enable(enable);
        return();
    }

    let node: *rend_node = null;
    // insert before the node with the larger timeout
    list_foreach_entry!(node, &rend_timeout_list, rend_node, timeout_link, {
                if (node->timeout > rend_insert->timeout) {
                    list_insert_before(&rend_insert->timeout_link, &node->timeout_link); 
                    cond_preempt_enable(enable);
                    return();
                }
            });
    // if there exists no larger node, insert at the end 
    list_insert_tail(&rend_insert->timeout_link, &rend_timeout_list);

    cond_preempt_enable(enable);
}

/* invoked by timer handler to wake up all nodes (with timeout error) whose
 * timeout times are smaller than timeout (IE have expired) */
fn rend_timeout_remove(timeout: u32) {
    let enable = cond_preempt_disable();
    let node: *rend_node = null;
    // insert before the node with the larger timeout
    let node_prev: *rend_node = null;
    list_foreach_entry!(node, &rend_timeout_list, rend_node, timeout_link, {
            // wake up previous link (we don't do current so we can reset all
            // the pointers and such)
            if (node_prev != null) {
                if (node_prev->can_timeout) {
                    list_del(&node_prev->rend_link); 
                    rend_signal(null, 0, node_prev, REND_ERR_TIMEOUT);
                }
                node_prev = null;
            }
                if (node->timeout > timeout) { 
                    break;    
                } 
                node_prev = node;
        });

    if (node_prev != null && node_prev->can_timeout) {
        list_del(&node_prev->rend_link); 
        rend_signal(null, 0, node_prev, REND_ERR_TIMEOUT);
    }

    cond_preempt_enable(enable);
}

/* if an opposing node exists, grab it atomically and return it.  else, go to sleep
 * and wait for an opposing node to do the action and return null; in this case,
 * *message gets set to the message of the opposing node; else, message is null 
 */
fn rend_wait(rend: *rend, message: *u32, message_len: u32, message_recv: **u32,
        recv_len: *u32, timeout: u32, option: u32, err: *i32) -> *rend_node {

    if (!rend->rend_active) {
        return null
    }

    let other_head: *list_head;
    let own_head: *list_head; 

    let my_node: *rend_node; 
    // if message as asynchronous, allocate a node rather than placing it on the
    // stack, and copy the message to a dynamic buffer in case the invoker
    // placed it on the stack 
    if (option != SEND_ASYNC) {
        let stack_node: rend_node;
        my_node = &stack_node;
    } else {
        my_node = slub_alloc(sizeof(rend_node)) as *rend_node;
        if (my_node == null) {
            *err = REND_ERR_MEM;
            return null;  
        }
        let new_message = slub_alloc(message_len) as *u32;
        // TODO this policy is bad.  possible rend needs a better interface 
        if (new_message == null) {
            slub_free(my_node as *u32, sizeof(rend_node));
            *err = REND_ERR_MEM;

            return null 
        }
        mos_memcpy(new_message as *u8, message as *u8, message_len);
        message = new_message;
    }


    list_init_node(&my_node->timeout_link);
    list_init_node(&my_node->rend_link);


    if (option == SEND_SYNC || option == SEND_ASYNC || option ==
            SEND_SYNC_NOREPLY) {
        other_head = &rend->recv_head;
        own_head = &rend->send_head;
    } else {
        assert!(option == RECV);
        other_head = &rend->send_head;
        own_head = &rend->recv_head;
    }

    /* when a message is sent with option SEND_SYNC_REPLY, it requires that the
     * receiver perform the transfer.  In the case that a receiver is already
     * waiting when the sender calls rend_wait, it wakes up the receiver and
     * goes to sleep on its list to be woken.  The receiver then loops around
     * and attempts to collect the sender.  Potentially loops until rend is no
     * longer active, or the message times out 
     * TODO this design is a little hacky.  Some improvements to make it more
     * elegant would be nice. 
     * I'll be honest -- if i had gotos, I'd use gotos 
     */
    let replay_flag = true;
    while (replay_flag) {
        
        replay_flag = false;
        my_node->message = message;
        my_node->message_len = message_len;
        my_node->option = option;
        my_node->can_timeout = true;
        my_node->err = 0;
        my_node->timeout = timeout;

        let enable = cond_preempt_disable();

        if (!rend->rend_active) {

            cond_preempt_enable(enable);
            *err = REND_ERR_DEAD; 

            if (option == SEND_ASYNC) {
                slub_free(my_node as *u32, sizeof(rend_node));
                slub_free(message as *u32, message_len);
            }

            return null;
        } 

        /* see if there is another node on the other side to grab */
        if (!list_is_empty(other_head)) {
            let other_node: *rend_node = list_head_entry!(other_head,
                    rend_node, rend_link);
            assert!(other_node != null);
            other_node->can_timeout = false;
            list_del(&other_node->rend_link);

            if (option != SEND_SYNC_REPLY) {

                cond_preempt_enable(enable);
                *message_recv = other_node->message;

                if (option == SEND_ASYNC) {
                    slub_free(my_node as *u32, sizeof(rend_node));
                    slub_free(message, message_len);
                }


                *err = 0;

                assert!(!in_list(&my_node->timeout_link));
                assert!(in_list(&other_node->timeout_link) || other_node->timeout ==
                        -1);

                cond_preempt_enable(enable);
                return other_node; // we allocate this on the stack, but it's asleep and direct-mapped, so we're ok
            } else {
                rend_signal(null, 0, other_node, REND_ERR_REPLAY);
            }
        } 

        my_node->tcb = get_tcb();

        list_insert_tail(&my_node->rend_link, own_head); 
        rend_timeout_insert(my_node);
        if (option != SEND_ASYNC) {
            scheduler_update(STATE_COND as i32);
            if (in_list(&my_node->timeout_link)) {
                list_del(&my_node->timeout_link);
            }
            if (my_node->err == REND_ERR_REPLAY as u32) {
                // if we are signaled with this error, it means a send needs us
                // to perform the transfer.  We must wake up and collect it
                replay_flag = true;
                assert!(option == RECV);
            }
        }

        /* while if we are going through the loop again, it means sending thread
         * has awoken us and gone to sleep in the hopes that we would wake up
         * and collect it, it's possible another receiver got to it before us.
         * Thus, we reenable preemption for the loop case as well, to avoid
         * spending large amounts of time with preemption disabled, (though each
         * time we fail, we do call scheduler_update(), so this is arguably
         * not a proper concern) 
         */
        cond_preempt_enable(enable);
    }
    *message_recv = my_node->message;
    *recv_len = my_node->message_len;
    *err = my_node->err as i32;

    assert!(!in_list(&my_node->timeout_link));
    null
}


/* give new message to rend_node and wake it up */
/* makes no reference to original rend in case it was removed */
fn rend_signal(message: *u32, message_len: u32, rend_node: *rend_node, err: i32) {

    if (rend_node->option == SEND_ASYNC) {

        if (rend_node->message != null) {
            slub_free(rend_node->message, rend_node->message_len); // TODO not this 
        }
        slub_free(rend_node as *u32, sizeof(rend_node));
        return ();
    } 

    rend_node->message = message;
    rend_node->message_len = message_len;
    rend_node->err = err as u32;
    let enable = cond_preempt_disable();
    if (in_list(&rend_node->timeout_link)) {
        list_del(&rend_node->timeout_link);
    }
    rend_node->tcb->state = STATE_RUNNABLE;
    schedule(rend_node->tcb, QUEUE_FRONT);


    // take ourselves off the run queue to be awoken with reply?
    cond_preempt_enable(enable);
}

/* Cleans up a dying rend: wakes up all waiting threads (with error) */
fn rend_destroy(rend: *rend, err: i32) {
    rend->rend_active = false;

    while(!list_is_empty(&rend->send_head) ) {
        let wakeup_node = list_head_entry!(&rend->send_head,
                rend_node, rend_link);
        list_del(&(wakeup_node->rend_link));

        rend_signal(null, 0, wakeup_node, err as i32);  
    }

    while(!list_is_empty(&rend->recv_head) ) {
        let wakeup_node = list_head_entry!(&rend->recv_head,
                rend_node, rend_link);
        list_del(&(wakeup_node->rend_link));

        rend_signal(null, 0, wakeup_node, err as i32);  
    }

}
