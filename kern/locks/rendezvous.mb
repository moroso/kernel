use locks::mutex::*;
use structures::tcb::*;
use utils::list::*;
use structures::schedule::*;
use consts::*;
use utils::slab_alloc::*;
use utils::stdlib::*;

/* sync options */
const SEND_SYNC_REPLY: u32 = 0;
const SEND_SYNC: u32 = 0;
const SEND_ASYNC: u32 = 2;
const SEND_SYNC_NOREPLY: u32 = 3;
const RECV: u32 = 1;

/* error options */
const ERR_MEM: i32 = -1;
const ERR_TIMEOUT: i32 = -2;
const ERR_DEAD: i32 = -3;
const ERR_REPLAY: i32 = -4;

struct rend_node {
    message: *u32,
    message_len: u32,
    tcb: *tcb,
    timeout: u32,
    timeout_link: list_node,
    rend_link: list_node,
    option: u32,
    err: u32,
}

struct rend {
    send_head: list_head,
    recv_head: list_head,
    rend_active: bool,
    last_send: *tcb,
}

fn rend_init(rend: *rend) {
    list_init_head(&rend->recv_head);
    list_init_head(&rend->send_head);
    rend->rend_active = true;
}

/* if an opposing node exists, grab it atomically and return it.  else, go to sleep
 * and wait for an opposing node to do the action and return null; in this case,
 * *message gets set to the message of the opposing node; else, message is null 
*/
fn rend_wait(rend: *rend, message: *u32, message_len: u32, message_recv: **u32,
        recv_len: *u32, timeout: u32, option: u32, err: *i32) -> *rend_node {

    let other_head: *list_head;
    let own_head: *list_head; 

    let my_node: *rend_node; 
    if (option != SEND_ASYNC) {
        let stack_node: rend_node;
        my_node = &stack_node;
    } else {
        my_node = slub_alloc(sizeof(rend_node)) as *rend_node;
        if (my_node == null) {
            *err = ERR_MEM;
            return null;  
        }
        let new_message = slub_alloc(message_len) as *u32;
        // TODO this policy is bad.  possible rend needs a better interface 
        if (new_message == null) {
            slub_free(my_node as *u32, sizeof(rend_node));
            *err = ERR_MEM;

            return null 
        }
        mos_memcpy(new_message as *u8, message as *u8, message_len);
        message = new_message;
    }

    my_node->message = message;
    my_node->message_len = message_len;
    my_node->option = option;
    my_node->err = 0;

    if (option == SEND_SYNC || option == SEND_ASYNC || option ==
            SEND_SYNC_NOREPLY) {
        other_head = &rend->recv_head;
        own_head = &rend->send_head;
    } else {
        assert!(option == RECV);
        other_head = &rend->send_head;
        own_head = &rend->recv_head;
    }

    let enable = cond_preempt_disable();

    my_node->timeout = timeout; // TODO update timeout


    if (!rend->rend_active) {
        *err = ERR_DEAD; 
        if (option == SEND_ASYNC) {
            slub_free(my_node as *u32, sizeof(rend_node));
            slub_free(message as *u32, message_len);
        }
        return null;
    }

    /* see if there is another node on the other side to grab */
    if (!list_is_empty(other_head)) {
        let other_node: *rend_node = list_head_entry!(other_head,
                rend_node, rend_link);
        assert!(other_node != null);
        list_del(&other_node->rend_link);
        if (option != SEND_SYNC_REPLY) {
            cond_preempt_enable(enable);
            *message_recv = other_node->message;
            if (option == SEND_ASYNC) {
                slub_free(my_node as *u32, sizeof(rend_node));
                slub_free(message, message_len);
            }
            *err = 0;
            cond_preempt_enable(enable);

            return other_node; // we allocate this on the stack, but it's asleep and direct-mapped, so we're ok
        } else {
            rend_signal(null, 0, other_node, ERR_REPLAY);
        }
    } 

    if (option != SEND_ASYNC) {
        my_node->tcb = get_tcb();
    }

    // TODO timeout queue
    list_insert_tail(&my_node->rend_link, own_head);

    if (option != SEND_ASYNC) {
        //scheduler_update(STATE_COND as i32);
        let buffer: u32[5];
        fp_recv(buffer);
        if (my_node->err == ERR_REPLAY as u32) {
            // TODO this is sorta bad.  it is made more ok by the guarantee that
            // we recurse only once, and there are no dynamic mem
            // allocations to worry about
            assert!(option == RECV);
            cond_preempt_enable(enable);
            return rend_wait(rend, message, message_len, message_recv,
                    recv_len, timeout, option, err);
        }
    }

    *message_recv = my_node->message; 
    *recv_len = my_node->message_len;
    *err = my_node->err as i32;
    cond_preempt_enable(enable);

    if (option == RECV) {
        printf!("halp\n");
       // assert!(!list_is_empty(own_head)); 
    }

    null
}


/* give new message to rend_node and wake it up */
fn rend_signal(message: *u32, message_len: u32, rend_node: *rend_node, err: i32) {
    if (rend_node->tcb == null) {
        if (rend_node->message != null) {
            slub_free(rend_node->message, rend_node->message_len); // TODO not this 
        }
        slub_free(rend_node as *u32, sizeof(rend_node));
        return ();
    }

    rend_node->message = message;
    rend_node->message_len = message_len;
    rend_node->err = err as u32;

    let enable = cond_preempt_disable();
    //rend_node->tcb->state = STATE_RUNNABLE;
    let buffer: u32[5];
    fp_send(rend_node->tcb, buffer); 

    // take ourselves off the run queue to be awoken with reply?
    cond_preempt_enable(enable);
}

fn rend_destroy(rend: *rend, err: i32) {
    rend->rend_active = false;

    /* we should be able to do this with preemption enabled because no more
     * senders are going to try to send and we are the only one that receives */

    while(!list_is_empty(&rend->send_head) ) {
        let wakeup_node = list_head_entry!(&rend->send_head,
                rend_node, rend_link);
        list_del(&(wakeup_node->rend_link));
        rend_signal(null, 0, wakeup_node, err as i32);  
    }
}
