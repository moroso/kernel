/* Memory allocator -- whee ("slab allocator")
 * a very simple slab allocator.  the basic allocation function
 * searches a list of sorted, sized "caches" for the bucket size that is the
 * closest fit to the allocation.
 *
 *   each cache contains a list of elements cut
 * from one or more "slabs": large, contiguous chunks of memory handed to us by
 * the buddy allocator
 *
 * users can also create caches for memory chunks of uniform size; however, we
 * are not currently preserving struct initialization
 *
 * In order for this to actually work, we will need globals
 * TODO remove slabs when they are freed, object caches, 
 * Author: Amanda M. Watson
 */


use consts::*;
use utils::list::*;
use utils::buddy_alloc::{frame_alloc,frame_alloc_pow2};
use locks::mutex::*;
use structures::schedule::*;


const debug: bool = true;
const SLAB_SIZE: u32 = PAGE_SIZE; /* this really isn't the right size*/
const UINT_SIZE: u32 = 4;
const POWER_MIN: u32 = 2;
const POWER_MAX: u32 = 9;


struct slab_descriptor {
    slab_low: *u32,
    slab_link: list_node,
}

struct cache_head {
    size: u32,
    slab: *u32,
    link: list_node,
    mut: kmut,
    slab_list: list_head,
}

static head: list_head = LIST_HEAD_INIT!(head);
static caches: *cache_head = null;

fn slab_add(size: u32, cache: *cache_head) {
    let slab: *u32 = frame_alloc() as *u32;
    if (slab == null) {
        return ();
    }
    let desc: *slab_descriptor = slab as *slab_descriptor;
    slab+=sizeof(slab_descriptor);
    desc->slab_low = slab;
    list_init_node(&(desc->slab_link)); 
    list_insert_head(&desc->slab_link, &cache->slab_list);

    /* divide into elements */
    let node_prev: *u32 = slab;
    let i: u32;
    for (i = size; i < (SLAB_SIZE - sizeof(slab_descriptor)); i+=size) {
        let node: *u32 = ((node_prev as u32) + size) as *u32;
        *node_prev = node as u32;
        node_prev = node;
    }
    *node_prev = 0;
    let node: *u32 = slab;
    /* ensure that we have the correct number of elements and that they are each
     * an offset of size away from each other */
    i = 0;
    while (node != null) {
        assert!(!(*node - (node as u32) != size && *node != 0));

        node = *node as *u32;
        i+=1;
    };

    /* should divide evenly */
    //assert!(i == ((SLAB_SIZE - sizeof(slab_descriptor))/size));
    cache->slab = slab;

}

/* returns the index of the correct cache for the given size */
fn find_cache(size: u32) -> i32 {
    let power: u32;
    for (power = POWER_MIN; power <= POWER_MAX; power +=1) {
        if (size <= (1 << power)) {
            return (power - POWER_MIN) as i32;
        }
    }
    -1
}

/* creates a cache with a single slab, full of size-sized elements
 * We expect that size is a power of two
 */
fn cache_create(size: u32, cache: *cache_head) -> i32 {
    cache->size = size;
    list_init_head(&cache->slab_list);
    kmut_init(&cache->mut);
    slab_add(size, cache);
    if (cache->slab == null) {
        return -1;
    }
    0
}

/* remove an element from the given cache.  if cache is empty, a new slab is
 * allocated  */
fn cache_remove (cache: *cache_head) -> *u32 {

    kmut_lock(&cache->mut);
    if (cache->slab == null) {
        slab_add(cache->size, cache);
    }

    let element: *u32 = cache->slab;
    if (element != null) {
        cache->slab = *element as *u32;

    }
    kmut_unlock(&cache->mut);
    element
}

/* add element back into cache */
fn cache_add (element: *u32, cache: *cache_head) {
    /* TODO check memory bounds */
    kmut_lock(&cache->mut);
    *element = cache->slab as u32;
    cache->slab = element;
    let slab_node: *slab_descriptor = null;
    let found: bool = false;
    if (debug) {
        list_foreach_entry!(slab_node, &(cache->slab_list), slab_descriptor,
                slab_link, {
                if (element > slab_node->slab_low && element < slab_node->slab_low +
                    SLAB_SIZE - sizeof(slab_descriptor)) {
                found = true;
                break;
                }}); 
        if (!found) {
            printf!("error freeing %p\n", element);
            assert!(false);
        }
    }
    kmut_unlock(&cache->mut);
}

fn slub_init() {
    // we are doing this sham of an allocation because we can't have a sized
    // array as a global, due to the compiler's initialization requirement.
    // Would like if fix.
    caches = frame_alloc() as *cache_head;
    let i: u32;
    for (i = POWER_MIN; i <= POWER_MAX; i+=1) {
        let cache = &caches[i - POWER_MIN];
        cache_create(1 << i, cache);
    }
}

/* NOTE: does NOT support sizes larger than a slab.  For that, you will want to
 * use frame_alloc */
fn slub_alloc(size: u32) -> *u32 {

    let cache_num: i32 = find_cache(size);
    if (cache_num == -1) {
        return null;
    }
    cache_remove(&(caches[cache_num as u32]))
}

fn slub_free(element: *u32, size: u32) {
    // ensure we aren't freeing something on our thread's stack
    assert!(!((element as u32) > ((get_tcb()->kstack_top as u32) + 2 * PAGE_SIZE as u32) && (element as u32) <
                (get_tcb()->kstack_top as u32)));
    if (size == 0) {
        return ();
    }
    let cache_num: i32 = find_cache(size);
    if (cache_num == -1) {
        return ();
    }
    cache_add(element, &(caches[cache_num as u32]))
}
