/* ipc/mod.mb
 * Contains implementation of ipc request primitives, such as send and recv 
 *
 * Author: Amanda M. Watson
*/

use structures::tcb_dir::*;
use structures::tcb::*;
use structures::proc_dir::*;
use locks::mutex::*;
use locks::cond::*;
use consts::STATE_DEAD;
use structures::schedule::*;
use structures::VM::{is_user_addr, page_transfer, log_to_phys, is_addr_aligned};
use locks::rendezvous::*;
use consts::*;
use utils::stdlib::*;
use utils::slab_alloc::*;
use utils::string::*;
use utils::user_mem::*;
use utils::list::*;
use entry::get_ticks;
use structures::mailbox::*;
use shared::ipc_defs::*;
mod desc_table;

struct message_node {
    long_src: buffer_t,
    long_dest: buffer_t, 
    short_src: buffer_t,
    options: u32,
    err: u32,
}

// size of buffer we're willing to put on the stack for short messages
const SHORT_BUFFER_MAX: u32 = 200;

fn ipc_send(args: *ipc_args) -> i32 {
    
    /* copying arguments into the kernel */   
    let args_kern: ipc_args;
    if (copy_from_user((&args_kern) as *u8, args as *u8, sizeof(ipc_args)) < 0) {
        return IPC_ERR_INTERNAL;
    } 

    /* identifying send options for internal IPC */ 
    let option: u32 = SEND_SYNC_REPLY;
    if (args_kern.options&OPTION_ASYNC != 0) {
        option = SEND_ASYNC;
    } else if (args_kern.options&OPTION_NO_REPLY != 0) {
        option = SEND_SYNC_NOREPLY;
    }

    let shared: u32 = args_kern.options&OPTION_SHARE;

    /* if the message is short and needs a reply (meaning we're asleep until the
     * receiver copies us to userspace), place a manageable short message on the
     * stack.  Otherwise, dynamically allocate (we can save this for noreplies
     * when we do the fast path) */ 
    let short_message: *u32 = null;
    if (args_kern.short_src.len > 0) {
        if (option == SEND_SYNC_REPLY && args_kern.short_src.len <
                SHORT_BUFFER_MAX) {
            let short_message_temp: u32[SHORT_BUFFER_MAX];
            short_message = &short_message_temp[0];
        } else {
            short_message = slub_alloc(args_kern.short_src.len); 
        }
        if (short_message == null || copy_from_user(short_message as *u8,
                    args_kern.short_src.addr,
                    args_kern.short_src.len) < 0) {
            return IPC_ERR_INTERNAL;
        }
    }


    // check to see if addresses all point to user addresses
    if ((!is_user_addr(args_kern.long_src.addr) && args_kern.long_src.len > 0) ||
            (!is_user_addr(args_kern.long_dest.addr)  && args_kern.long_dest.len > 0)  ||
            (!is_user_addr(args_kern.short_src.addr)  && args_kern.short_src.len > 0) ||
            (!is_user_addr(args_kern.short_dest.addr))  && args_kern.short_dest.len > 0)  {
        return IPC_ERR_ARGS;  
    }

    /* ensures long addresses are page aligned */
    if ((!is_addr_aligned(args_kern.long_src.addr) && args_kern.long_src.len > 0)||
            (!is_addr_aligned(args_kern.long_dest.addr)) && args_kern.long_dest.len > 0)  {
        return IPC_ERR_ARGS;  
    }

    /* ensures short message is smaller than page */
    if (args_kern.short_src.len > PAGE_SIZE) {
        return IPC_ERR_ARGS;
    }

    /* ensures long lengths are multiples of page sizes */
    if (args_kern.long_src.len%PAGE_SIZE != 0 ||
            args_kern.long_dest.len%PAGE_SIZE != 0)
    {
        return IPC_ERR_ARGS;
    }    

    /* looks up descriptor for dest mailbox */
    let dest_mb = desc_table::get_ipc_dest(args_kern.dest_id);

    if dest_mb == null {
        return IPC_ERR_DEST;
    }

    /*** create message that relays ipc info to receiver ***/
    let message_send: message_node;
    mos_memcpy(&message_send.long_src, &args_kern.long_src, sizeof(buffer_t));
    mos_memcpy(&message_send.long_dest, &args_kern.long_dest, sizeof(buffer_t));
    message_send.short_src.addr = short_message as *u8;
    message_send.short_src.len = args_kern.short_src.len;
    message_send.options = args_kern.options;

    let message_recv: *u8;
    let recv_len: u32;
    let err: i32;

    let dest_tcb = mb_send(dest_mb, 
            &message_send as *u32, sizeof(message_node),
            &message_recv as **u32,
            &recv_len, 
            args_kern.timeout, option, &err);  

    /*** if there was an error during the process, return ***/
    if (err < 0) {

        if (err == REND_ERR_TIMEOUT) {
            return IPC_ERR_TIMEOUT;
        }

        // if tcb died before we could transfer, return with error
        if (err == REND_ERR_DEAD) {

            return IPC_ERR_DEAD;
        }
        return IPC_ERR_INTERNAL;
    }

    // if we have no message and no error, this means the full transfer
    // occurred on the other side; clean up and return
    if (recv_len == 0) {

        /* receiver already transferred long message, so update length */
        args_kern.long_dest.len = message_send.long_dest.len;

        /* copy receiver's short reply to userland */
        args_kern.short_dest.len = min(recv_len, args_kern.short_dest.len);
        if (copy_to_user((args_kern.short_dest.addr), message_recv as *u8,
                    min(recv_len, args_kern.short_dest.len)) <
                0) {
            return IPC_ERR_INTERNAL;
        }

    } else {

        /* if we have a message, this means the receiver is sleeping, and we
         * perform the transfer ourselves */
        let message_dest: *message_node = message_recv as *message_node;
        if (message_dest->long_dest.len > 0) {

            assert!(dest_tcb != null);
            /* in our current system, all active tcbs must have an active proc.
             * Thus, we don't need to increment the ref count  */
            let proc_recv = dest_tcb->proc;
            assert!(proc_recv != null);

            /*** transfer long message ***/
            let transfer_len = min!(args_kern.long_src.len, message_dest->long_dest.len);
            kmut_2lock(&(get_tcb()->proc->PD.mut), &(proc_recv->PD.mut));

            let page: u32;
            for (page = 0; page < transfer_len; page+=PAGE_SIZE) {
                if (page_transfer(args_kern.long_src.addr + page,
                            message_dest->long_dest.addr + page,
                            &(get_tcb()->proc->PD),
                            &(proc_recv->PD), shared) < 0) {
                    transfer_len = max(0, page - PAGE_SIZE);
                    break;
                }
            }
            kmut_2unlock(&(get_tcb()->proc->PD.mut), &(proc_recv->PD.mut));
            message_dest->long_dest.len = transfer_len;
        } 

        /*** transfer short message and wake up receiver ***/
        mb_end_send(short_message, args_kern.short_src.len, 0);
    }

    if (copy_to_user(args as *u8, &args_kern as *u8, sizeof(ipc_args)) <
            0) {

        return IPC_ERR_INTERNAL;
    }

    0
}

/* 
 * receives a number of pages up to len_long bytes, mapped in starting with address
 * long_dest.  If a sender is waiting to send, retrieves their info and performs
 * the transfer.  Otherwise, deschedules until a sender becomes available and
 * does the transfer itself.  Returns 0 on success,
 * negative error code on failure
 *
 * Requires long_dest be a userland address
 *
 * long_dest: userspace address where new pages are to be mapped in
 * message: buffer for a message
 * len_short: length of message 
 * timeout: time (in ms) before we give up waiting and return failure
 */
fn ipc_recv(args: *ipc_args) -> i32 {
    let args_kern: ipc_args;
    let enable = cond_preempt_disable();
    if (!list_is_empty(&get_tcb()->last_send)) {
        cond_preempt_enable(enable);
        return IPC_ERR_REPLY;
    }
    cond_preempt_enable(enable);

    if (copy_from_user((&args_kern) as *u8, args as *u8, sizeof(ipc_args)) < 0) {
        return IPC_ERR_INTERNAL;
    }

    // check to see if addresses all point to user addresses
    if ((!is_user_addr(args_kern.long_dest.addr)  && args_kern.long_dest.len > 0)  ||
            (!is_user_addr(args_kern.short_dest.addr))  && args_kern.short_dest.len > 0)  {
        return IPC_ERR_ARGS;  
    }

    // ensures long addresses are page aligned
    if ((!is_addr_aligned(args_kern.long_dest.addr)) && args_kern.long_dest.len > 0)  {
        return IPC_ERR_ARGS;  
    }

    // ensures long lengths are multiples of page sizee 
    if (args_kern.long_dest.len%PAGE_SIZE != 0)
    {
        return IPC_ERR_ARGS;
    }    

    let message_send: message_node;
    mos_memcpy(&message_send.long_dest, &args_kern.long_dest, sizeof(buffer_t));

    let message_recv: *u8;
    let recv_len: u32;
    let err: i32;
    let timeout = args_kern.timeout + get_ticks();
    if (args_kern.timeout == -1) {
        timeout = -1;
    }

    let dest_mb = desc_table::get_ipc_dest(args_kern.dest_id);
    if (dest_mb == null) {
        return IPC_ERR_DEST;
    } 

    let dest_tcb = mb_recv(dest_mb, 
            &message_send as *u32, sizeof(message_node),
            &message_recv as **u32, &recv_len, 
            timeout, &err);  

    if (err == REND_ERR_TIMEOUT) {
        return IPC_ERR_TIMEOUT;
    }

    // if tcb died before we could transfer, return with error
    if (err == REND_ERR_DEAD) {
        return IPC_ERR_DEAD;
    }

    if (err < 0) {
        return IPC_ERR_INTERNAL;
    }

    // if we have no rend_node and no error, this means the full transfer
    // occurred; clean up and return
    if (recv_len == 0) {
        // new length written into message (sender updates this length)
        args_kern.long_dest.len = message_send.long_dest.len;
        args_kern.short_dest.len = recv_len;
        if (copy_to_user((args_kern.short_dest.addr), message_recv as *u8,
                    min(recv_len, args_kern.short_dest.len)) <
                0) {
            return IPC_ERR_INTERNAL;
        }
    } else {

        let message_src: *message_node = message_recv as *message_node;
        let transfer_len = min!(message_src->long_src.len, args_kern.long_dest.len);
        if (transfer_len > 0) {
            assert!(dest_tcb != null);
            kmut_2lock(&(get_tcb()->proc->PD.mut), &(dest_tcb->proc->PD.mut));
            let page: u32;
            for (page = 0; page < transfer_len; page+=PAGE_SIZE) {
                if (page_transfer(message_src->long_src.addr + page,
                            args_kern.long_dest.addr + page,
                            &(dest_tcb->proc->PD), 
                            &(get_tcb()->proc->PD),
                            message_src-> options&OPTION_SHARE) < 0) {
                    transfer_len = max(0, page - PAGE_SIZE);
                    break;
                }
            }
            args_kern.long_dest.len = transfer_len;
            kmut_2unlock(&(get_tcb()->proc->PD.mut), &(dest_tcb->proc->PD.mut));

        }
        if (copy_to_user((args_kern.short_dest.addr),
                    message_src->short_src.addr as *u8,
                    min(message_src->short_src.len, args_kern.short_dest.len)) <
                0) {

            mb_end_recv(IPC_ERR_INTERNAL);
            return IPC_ERR_INTERNAL;
        }
        if (message_src->short_src.len > 0 &&
                message_src->options&(OPTION_NO_REPLY | OPTION_ASYNC) == 0 &&
                message_src->short_src.len > SHORT_BUFFER_MAX) {
            slub_free(message_src->short_src.addr as *u32,
                    message_src->short_src.len);
        } 

        mb_end_recv(0);
    }

    if (copy_to_user(args as *u8, &args_kern as *u8, sizeof(ipc_args)) <
            0) {

        return IPC_ERR_INTERNAL;
    }

    0
}


/* replies to the last sender */
fn reply(message_long: *u8, long_len: u32, message_short: *u32, short_len: u32) -> i32 {

    let enable = cond_preempt_disable();
    if (list_is_empty(&get_tcb()->last_send)) {
        cond_preempt_enable(enable);
        return IPC_ERR_REPLY;
    }

    let last_send: *rend_node = list_head_entry!(&get_tcb()->last_send, rend_node,
            rend_link);
    assert!(last_send != null);
    last_send->can_timeout = false;
    list_del(&last_send->rend_link);
    cond_preempt_enable(enable);

    let message_src: *message_node = last_send->message as *message_node;


    if (message_src->long_dest.len > 0) {

        let transfer_len = min!(message_src->long_dest.len, long_len);
        kmut_2lock(&(get_tcb()->proc->PD.mut), &(last_send->tcb->proc->PD.mut));
        let page: u32;
        for (page = 0; page < transfer_len; page+=PAGE_SIZE) {
            if (page_transfer(message_long + page,
                        message_src->long_dest.addr + page,
                        &(get_tcb()->proc->PD),
                        &(last_send->tcb->proc->PD), 0) < 0) {
                transfer_len = max(0, page - PAGE_SIZE);
                break;
            }
        }
        kmut_2unlock(&(get_tcb()->proc->PD.mut), &(last_send->tcb->proc->PD.mut));

    }

    let message_kernel = null;
    if (short_len > 0) {
        message_kernel = slub_alloc(short_len);
        if (message_kernel == null || copy_from_user(message_kernel as *u8,
                    message_short as *u8,
                    short_len) < 0) {


            rend_signal(message_kernel, short_len, last_send, 0);
            return IPC_ERR_INTERNAL;
        }
    }

    rend_signal(message_kernel, short_len, last_send, 0);

    0
}


