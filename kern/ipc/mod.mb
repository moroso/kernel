/* ipc/mod.mb
 * Contains implementation of ipc request primitives, such as send and recv 
 *
 * Author: Amanda M. Watson
*/

use structures::tcb_dir::*;
use structures::tcb::*;
use structures::proc_dir::*;
use locks::mutex::*;
use locks::cond::*;
use consts::STATE_DEAD;
use structures::schedule::*;
use structures::VM::{is_user_addr, page_transfer, log_to_phys, is_addr_aligned};
use locks::rendezvous::*;
use consts::*;
use utils::stdlib::*;
use utils::slab_alloc::*;
use utils::string::*;
use utils::user_mem::*;
use utils::list::*;
use entry::get_ticks;
use structures::mailbox::*;
use shared::ipc_defs::*;
mod desc_table;

struct message_node {
    long_src: buffer_t,
    long_dest: buffer_t, 
    short_src: buffer_t,
    options: u32,
    err: u32,
}

// size of buffer we're willing to put on the stack for short messages
const SHORT_BUFFER_MAX: u32 = 200;

/*
 * given a valid source buffer and an active tid, send a message to thread
 *  if sending tcb is called while
 * tcb id is waiting, it will perform the transfer itself. If not, it
 * relies on the receiver to perform the
 * transfer.  
 * 
 * The current modes of message passing are the following:
 *  - Long IPC: this is synchronous only.  Sends a page range starting at long_src
 *  and extending to len_long.  If receiver is not currently waiting, thread
 *  deschedules until receiver dies or the receiver sends a reply.
 *  Otherwise, transfers the pages into the receiver's VM and wake up receiver.
 *  Original mapping is removed.  Also sends a short
 *  message message with length len_short.
 * - Short IPC: both synchronous and asynchronous.  A synchronous send acts just
 * like long IPC, but without mapping pages into the VM.  In async, if a
 * receiver is blocked, messages between sender and receiver are exchanged
 * immediately.  if no receiver is available, the message is left for the
 * receiver to pick up later, and sneder returns without a return message. 
 *
 * Requires that long_src be mapped in and be a user address. Requires that
 * len_short be less than PAGE_SIZE
 * Transfer removes mapping ar addr src and stores the frames at long_dest
 * Returns 0 on synchronous success, 1 if message was asynchronous.  A negative error
 * value is returned on error.
 *
 * id: currently the TID of the receiving tcb
 * long_src: the address at which the starting soure frame is mapped
 * message: short message to receiver
 * len_short: length of message
 * len_long: maximum length of transfer in bytes
 * timeout: time (in ms) before we return failure 
 */
fn ipc_send(args: *ipc_args) -> i32 {
    let args_kern: ipc_args;
    if (copy_from_user((&args_kern) as *u8, args as *u8, sizeof(ipc_args)) < 0) {
        return IPC_ERR_INTERNAL;
    } 

    let option: u32 = SEND_SYNC_REPLY;
    if (args_kern.options&OPTION_ASYNC != 0) {
        option = SEND_ASYNC;
    } else if (args_kern.options&OPTION_NO_REPLY != 0) {
        option = SEND_SYNC_NOREPLY;
    }

    let shared: u32 = args_kern.options&OPTION_SHARE;

    /* if the message is short and needs a reply (meaning we're asleep until the
     * receiver copies us to userspace), place a manageable short message on the
     * stack.  Otherwise, dynamically allocate (we can save this for noreplies
     * when we do the fast path) */ 
    let short_message: *u32 = null;
    if (args_kern.short_src.len > 0) {
        if (option == SEND_SYNC_REPLY && args_kern.short_src.len <
                SHORT_BUFFER_MAX) {
            let short_message_temp: u32[SHORT_BUFFER_MAX];
            short_message = &short_message_temp[0];
        } else {
            // TODO make this go on the stack if we can do fastpath
            short_message = slub_alloc(args_kern.short_src.len); 
        }
        if (short_message == null || copy_from_user(short_message as *u8,
                    args_kern.short_src.addr,
                    args_kern.short_src.len) < 0) {
            return IPC_ERR_INTERNAL;
        }
    }


    // check to see if addresses all point to user addresses
    if ((!is_user_addr(args_kern.long_src.addr) && args_kern.long_src.len > 0) ||
            (!is_user_addr(args_kern.long_dest.addr)  && args_kern.long_dest.len > 0)  ||
            (!is_user_addr(args_kern.short_src.addr)  && args_kern.short_src.len > 0) ||
            (!is_user_addr(args_kern.short_dest.addr))  && args_kern.short_dest.len > 0)  {
        return IPC_ERR_ARGS;  
    }

    // ensures long addresses are page aligned
    if ((!is_addr_aligned(args_kern.long_src.addr) && args_kern.long_src.len > 0)||
            (!is_addr_aligned(args_kern.long_dest.addr)) && args_kern.long_dest.len > 0)  {
        return IPC_ERR_ARGS;  
    }

    // ensures short message is smaller than page
    if (args_kern.short_src.len > PAGE_SIZE) {
        return IPC_ERR_ARGS;
    }

    // ensures long lengths are multiples of page sizee 
    if (args_kern.long_src.len%PAGE_SIZE != 0 ||
            args_kern.long_dest.len%PAGE_SIZE != 0)
    {
        return IPC_ERR_ARGS;
    }    

    // TODO talk about mailboxes, not tcbs
    let dest_mb = desc_table::get_ipc_dest(args_kern.dest_id);

    if dest_mb == null {

        return IPC_ERR_DEST;
    }

    let message_send: message_node;
    mos_memcpy(&message_send.long_src, &args_kern.long_src, sizeof(buffer_t));
    mos_memcpy(&message_send.long_dest, &args_kern.long_dest, sizeof(buffer_t));
    message_send.short_src.addr = short_message as *u8;
    message_send.short_src.len = args_kern.short_src.len;
    message_send.options = args_kern.options;

    let message_recv: *u8;
    let recv_len: u32;
    let err: i32;

    let timeout = args_kern.timeout + get_ticks();
    if (args_kern.timeout == -1) {
        timeout = -1;
    }
    let rend_node = mb_send(dest_mb, 
            &message_send as *u32, sizeof(message_node),
            &message_recv as **u32,
            &recv_len, 
            timeout, option, &err);  

    if (err == REND_ERR_TIMEOUT) {
        return IPC_ERR_TIMEOUT;
    }

    // if tcb died before we could transfer, return with error
    if (err == REND_ERR_DEAD) {

        return IPC_ERR_DEAD;
    }

    if (err < 0) {
        return IPC_ERR_INTERNAL;
    }



    // if we have no rend_node and no error, this means the full transfer
    // occurred; clean up and return
    if (rend_node == null) {
        // new length written into message (sender updates this length)
        args_kern.long_dest.len = message_send.long_dest.len;
        args_kern.short_dest.len = min(recv_len, args_kern.short_dest.len);
        if (copy_to_user((args_kern.short_dest.addr), message_recv as *u8,
                    min(recv_len, args_kern.short_dest.len)) <
                0) {
            return IPC_ERR_INTERNAL;
        }
    } else {
        // otherwise, we do the transfer ourselves and wake up the other guy
        let message_dest: *message_node = rend_node->message as *message_node;
        if (message_dest->long_dest.len > 0) {
            assert!(rend_node->tcb != null);
            let pid_recv = rend_node->tcb->pid;
            let proc_recv = proc_lookup(pid_recv);
            if (proc_recv == null || proc_recv->PD.PT_entries == null) {
                // TODO free short message stuff if errs
                mb_end_send(null, 0, IPC_ERR_DEAD);
                return IPC_ERR_DEAD;

            }

            let transfer_len = min!(args_kern.long_src.len, message_dest->long_dest.len);
            kmut_2lock(&(get_tcb()->proc->PD.mut), &(proc_recv->PD.mut));

            let page: u32;
            for (page = 0; page < transfer_len; page+=PAGE_SIZE) {
                if (page_transfer(args_kern.long_src.addr + page,
                            message_dest->long_dest.addr + page,
                            &(get_tcb()->proc->PD),
                            &(proc_recv->PD), shared) < 0) {
                    transfer_len = max(0, page - PAGE_SIZE);
                    break;
                }
            }
            kmut_2unlock(&(get_tcb()->proc->PD.mut), &(proc_recv->PD.mut));

            proc_end_lookup(proc_recv);
            message_dest->long_dest.len = transfer_len;
        } 
        mb_end_send(short_message, args_kern.short_src.len, 0);
    }

    if (copy_to_user(args as *u8, &args_kern as *u8, sizeof(ipc_args)) <
            0) {

        mb_end_send(null, 0, IPC_ERR_INTERNAL);
        return IPC_ERR_INTERNAL;
    }

    0
}

/* 
 * receives a number of pages up to len_long bytes, mapped in starting with address
 * long_dest.  If a sender is waiting to send, retrieves their info and performs
 * the transfer.  Otherwise, deschedules until a sender becomes available and
 * does the transfer itself.  Returns 0 on success,
 * negative error code on failure
 *
 * Requires long_dest be a userland address
 *
 * long_dest: userspace address where new pages are to be mapped in
 * message: buffer for a message
 * len_short: length of message 
 * timeout: time (in ms) before we give up waiting and return failure
 */
fn ipc_recv(args: *ipc_args) -> i32 {
    let args_kern: ipc_args;
    let enable = cond_preempt_disable();
    if (!list_is_empty(&get_tcb()->last_send)) {
        cond_preempt_enable(enable);
        return IPC_ERR_REPLY;
    }
    cond_preempt_enable(enable);

    if (copy_from_user((&args_kern) as *u8, args as *u8, sizeof(ipc_args)) < 0) {
        return IPC_ERR_INTERNAL;
    }

    // check to see if addresses all point to user addresses
    if ((!is_user_addr(args_kern.long_dest.addr)  && args_kern.long_dest.len > 0)  ||
            (!is_user_addr(args_kern.short_dest.addr))  && args_kern.short_dest.len > 0)  {
        return IPC_ERR_ARGS;  
    }

    // ensures long addresses are page aligned
    if ((!is_addr_aligned(args_kern.long_dest.addr)) && args_kern.long_dest.len > 0)  {
        return IPC_ERR_ARGS;  
    }

    // ensures long lengths are multiples of page sizee 
    if (args_kern.long_dest.len%PAGE_SIZE != 0)
    {
        return IPC_ERR_ARGS;
    }    

    let message_send: message_node;
    mos_memcpy(&message_send.long_dest, &args_kern.long_dest, sizeof(buffer_t));

    let message_recv: *u8;
    let recv_len: u32;
    let err: i32;
    let timeout = args_kern.timeout + get_ticks();
    if (args_kern.timeout == -1) {
        timeout = -1;
    }

    let dest_mb = desc_table::get_ipc_dest(args_kern.dest_id);
    if (dest_mb == null) {
        return IPC_ERR_DEST;
    } 

    let rend_node = mb_recv(dest_mb, 
            &message_send as *u32, sizeof(message_node),
            &message_recv as **u32, &recv_len, 
            timeout, &err);  

    if (err == REND_ERR_TIMEOUT) {
        return IPC_ERR_TIMEOUT;
    }

    // if tcb died before we could transfer, return with error
    if (err == REND_ERR_DEAD) {
        return IPC_ERR_DEAD;
    }

    if (err < 0) {
        return IPC_ERR_INTERNAL;
    }

    // if we have no rend_node and no error, this means the full transfer
    // occurred; clean up and return
    if (rend_node == null) {
        // new length written into message (sender updates this length)
        args_kern.long_dest.len = message_send.long_dest.len;
        args_kern.short_dest.len = recv_len;
        if (copy_to_user((args_kern.short_dest.addr), message_recv as *u8,
                    min(recv_len, args_kern.short_dest.len)) <
                0) {
            mb_end_recv(IPC_ERR_INTERNAL);
            return IPC_ERR_INTERNAL;
        }
    } else {

        let message_src: *message_node = rend_node->message as *message_node;
        let transfer_len = min!(message_src->long_src.len, args_kern.long_dest.len);
        if (transfer_len > 0) {

            kmut_2lock(&(get_tcb()->proc->PD.mut), &(rend_node->tcb->proc->PD.mut));
            let page: u32;
            for (page = 0; page < transfer_len; page+=PAGE_SIZE) {
                if (page_transfer(message_src->long_src.addr + page,
                            args_kern.long_dest.addr + page,
                            &(rend_node->tcb->proc->PD), 
                            &(get_tcb()->proc->PD),
                            message_src-> options&OPTION_SHARE) < 0) {
                    transfer_len = max(0, page - PAGE_SIZE);
                    break;
                }
            }
            args_kern.long_dest.len = transfer_len;
            kmut_2unlock(&(get_tcb()->proc->PD.mut), &(rend_node->tcb->proc->PD.mut));

        }
        if (copy_to_user((args_kern.short_dest.addr),
                    message_src->short_src.addr as *u8,
                    min(message_src->short_src.len, args_kern.short_dest.len)) <
                0) {

            mb_end_recv(IPC_ERR_INTERNAL);
            return IPC_ERR_INTERNAL;
        }
        if (message_src->short_src.len > 0 &&
                message_src->options&(OPTION_NO_REPLY | OPTION_ASYNC) == 0 &&
                message_src->short_src.len > SHORT_BUFFER_MAX) {
            slub_free(message_src->short_src.addr as *u32,
                    message_src->short_src.len);
        } 

    }

    if (copy_to_user(args as *u8, &args_kern as *u8, sizeof(ipc_args)) <
            0) {

        mb_end_recv(IPC_ERR_INTERNAL);
        return IPC_ERR_INTERNAL;
    }

    mb_end_recv(0);
    0
}


/* replies to the last sender */
fn reply(message_long: *u8, long_len: u32, message_short: *u32, short_len: u32) -> i32 {

    let enable = cond_preempt_disable();
    if (list_is_empty(&get_tcb()->last_send)) {
        cond_preempt_enable(enable);
        return IPC_ERR_REPLY;
    }

    let last_send: *rend_node = list_head_entry!(&get_tcb()->last_send, rend_node,
            rend_link);
    assert!(last_send != null);
    last_send->can_timeout = false;
    list_del(&last_send->rend_link);
    cond_preempt_enable(enable);

    let message_src: *message_node = last_send->message as *message_node;


    if (message_src->long_dest.len > 0) {

        let transfer_len = min!(message_src->long_dest.len, long_len);
        kmut_2lock(&(get_tcb()->proc->PD.mut), &(last_send->tcb->proc->PD.mut));
        let page: u32;
        for (page = 0; page < transfer_len; page+=PAGE_SIZE) {
            if (page_transfer(message_long + page,
                        message_src->long_dest.addr + page,
                        &(get_tcb()->proc->PD),
                        &(last_send->tcb->proc->PD), 0) < 0) {
                transfer_len = max(0, page - PAGE_SIZE);
                break;
            }
        }
        kmut_2unlock(&(get_tcb()->proc->PD.mut), &(last_send->tcb->proc->PD.mut));

    }


    let message_kernel = null;
    if (short_len > 0) {
        message_kernel = slub_alloc(short_len);
        if (message_kernel == null || copy_from_user(message_kernel as *u8,
                    message_short as *u8,
                    short_len) < 0) {


            rend_signal(message_kernel, short_len, last_send, 0);
            return IPC_ERR_INTERNAL;
        }
    }

    rend_signal(message_kernel, short_len, last_send, 0);

    0
}


