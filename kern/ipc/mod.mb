/* ipc/mod.mb
 * Contains implementation of ipc request primitives, such as send and recv 
 * Currently moves unshared pages
 * TODO timeouts, 
 *
 * Author: Amanda M. Watson
*/

use structures::tcb_dir::*;
use structures::tcb::*;
use structures::proc_dir::*;
use locks::mutex::*;
use locks::cond::*;
use consts::STATE_DEAD;
use structures::schedule::get_tcb;
use structures::VM::{is_user_addr, page_transfer, log_to_phys, is_addr_aligned};
use locks::rendezvous::*;
use consts::*;
use utils::stdlib::*;
use utils::slab_alloc::*;
use utils::string::*;
use utils::user_mem::*;
use utils::list::*;


extern fn get_cr3() -> u32;

struct message_node {
    long_src: buffer_t,
    long_dest: buffer_t, 
    short_src: buffer_t,
    options: u32,
    err: u32,
}

struct buffer_t {
    addr: *u8,
    len: u32,
}

const IPC_SHARE: u32 = 1; // shared memory
const OPTION_ASYNC: u32 = 2; // asynchronous send
const OPTION_NO_REPLY: u32 = 4; // sync, but no reply
const OPTION_SHARE: u32 = 8; // shared memory

struct ipc_args {
    dest_id: u32,
    long_src: buffer_t,
    long_dest: buffer_t, 
    short_src: buffer_t,
    short_dest: buffer_t,
    timeout: u32,
    options: u32,
}

// size of buffer we're willing to put on the stack for short messages
const SHORT_BUFFER_MAX: u32 = 200;
// maximum size of fastpath message
const FASTPATH_MAX: u32 = 100;

const ERR_INTERNAL: i32 = -1;
const ERR_DEST: i32 = -2;
const ERR_ARGS: i32 = -3;
const ERR_REPLY: i32 = -4;

fn get_ipc_dest(id: u32) -> *tcb {
    tcb_lookup(id)
}

/*
 * given a valid source buffer and an active tid, send a message to thread
 *  if sending tcb is called while
 * tcb id is waiting, it will perform the transfer itself. If not, it
 * relies on the receiver to perform the
 * transfer.  
 * 
 * The current modes of message passing are the following:
 *  - Long IPC: this is synchronous only.  Sends a page range starting at long_src
 *  and extending to len_long.  If receiver is not currently waiting, thread
 *  deschedules until receiver dies or the receiver sends a reply.
 *  Otherwise, transfers the pages into the receiver's VM and wake up receiver.
 *  Original mapping is removed.  Also sends a short
 *  message message with length len_short.
 * - Short IPC: both synchronous and asynchronous.  A synchronous send acts just
 * like long IPC, but without mapping pages into the VM.  In async, if a
 * receiver is blocked, messages between sender and receiver are exchanged
 * immediately.  if no receiver is available, the message is left for the
 * receiver to pick up later, and sneder returns without a return message. 
 *
 * Requires that long_src be mapped in and be a user address. Requires that
 * len_short be less than PAGE_SIZE
 * Transfer removes mapping ar addr src and stores the frames at long_dest
 * Returns 0 on synchronous success, 1 if message was asynchronous.  A negative error
 * value is returned on error.
 *
 * id: currently the TID of the receiving tcb
 * long_src: the address at which the starting soure frame is mapped
 * message: short message to receiver
 * len_short: length of message
 * len_long: maximum length of transfer in bytes
 * timeout: time (in ms) before we return failure 
*/
fn ipc_send(args: *ipc_args) -> i32 {
    let args_kern: ipc_args;
    if (copy_from_user((&args_kern) as *u8, args as *u8, sizeof(ipc_args)) < 0) {
        return ERR_INTERNAL;
    }
  
    let option: u32 = SEND_SYNC_REPLY;
    if (args_kern.options&OPTION_ASYNC != 0) {
        option = SEND_ASYNC;
    } else if (args_kern.options&OPTION_NO_REPLY != 0) {
        option = SEND_SYNC_NOREPLY;
    }

    let shared: u32 = args_kern.options&OPTION_SHARE;

    /* if the message is short and needs a reply (meaning we're asleep until the
     * receiver copies us to userspace), place a manageable short message on the
     * stack.  Otherwise, dynamically allocate (we can save this for noreplies
     * when we do the fast path) */ 
    let short_message: *u32 = null;
    if (args_kern.short_src.len > 0) {
        if (option == SEND_SYNC_REPLY && args_kern.short_src.len <
                SHORT_BUFFER_MAX) {
            let short_message_temp: u32[SHORT_BUFFER_MAX];
            short_message = &short_message_temp[0];
        } else {
            // TODO make this go on the stack if we can do fastpath
            short_message = slub_alloc(args_kern.short_src.len); 
        }
        if (short_message == null || copy_from_user(short_message as *u8,
                    args_kern.short_src.addr,
                    args_kern.short_src.len) < 0) {
            return ERR_INTERNAL;
        }
    }


    // check to see if addresses all point to user addresses
    if ((!is_user_addr(args_kern.long_src.addr) && args_kern.long_src.len > 0) ||
            (!is_user_addr(args_kern.long_dest.addr)  && args_kern.long_dest.len > 0)  ||
            (!is_user_addr(args_kern.short_src.addr)  && args_kern.short_src.len > 0) ||
            (!is_user_addr(args_kern.short_dest.addr))  && args_kern.short_dest.len > 0)  {
        return ERR_ARGS;  
    }

    // ensures long addresses are page aligned
    if ((!is_addr_aligned(args_kern.long_src.addr) && args_kern.long_src.len > 0)||
            (!is_addr_aligned(args_kern.long_dest.addr)) && args_kern.long_dest.len > 0)  {
        return ERR_ARGS;  
    }

    // ensures short message is smaller than page
    if (args_kern.short_src.len > PAGE_SIZE) {
        return ERR_ARGS;
    }

    // ensures long lengths are multiples of page sizee 
    if (args_kern.long_src.len%PAGE_SIZE != 0 ||
            args_kern.long_dest.len%PAGE_SIZE != 0)
    {
        return ERR_ARGS;
    }    

    // TODO talk about mailboxes, not tcbs
    let dest_tcb = get_ipc_dest(args_kern.dest_id);
    if dest_tcb == get_tcb() {
        tcb_end_lookup(dest_tcb);
        return ERR_DEST;
    }

    if dest_tcb == null {
        return ERR_DEST;
    }

    let message_send: message_node;
    mos_memcpy(&message_send.long_src, &args_kern.long_src, sizeof(buffer_t));
    mos_memcpy(&message_send.long_dest, &args_kern.long_dest, sizeof(buffer_t));
    message_send.short_src.addr = short_message as *u8;
    message_send.short_src.len = args_kern.short_src.len;
    message_send.options = args_kern.options;

    let static_buffer : u32[5];
    let message_recv: *u8 = &static_buffer[0] as *u8;
    let recv_len: u32;
    let err: i32;

    let rend_node = rend_wait(&(dest_tcb->ipc_rend), 
            &message_send as *u32, sizeof(message_node),
            &message_recv as **u32,
            &recv_len, 
            args_kern.timeout, option, &err);  

    // if tcb died before we could transfer, return with error
    if (err < 0) {
        tcb_end_lookup(dest_tcb);
        return ERR_DEST;
    }

    // if we have no rend_node and no error, this means the full transfer
    // occurred; clean up and return
    if (rend_node == null) {
        // new length written into message (sender updates this length)
        args_kern.long_dest.len = message_send.long_dest.len;
        args_kern.short_dest.len = min(recv_len, args_kern.short_dest.len);
        if (copy_to_user((args_kern.short_dest.addr), message_recv as *u8,
                    min(recv_len, args_kern.short_dest.len)) <
                0) {
            tcb_end_lookup(dest_tcb);
            return ERR_INTERNAL;
        }
    } else {
        // otherwise, we do the transfer ourselves and wake up the other guy
        let message_dest: *message_node = rend_node->message as *message_node;
        if (message_dest->long_dest.len > 0) {

            let pid_recv = dest_tcb->pid;
            let proc_recv = proc_lookup(pid_recv);
            if (proc_recv == null || proc_recv->PD.PT_entries == null) {
                // TODO free short message stuff of errs
                tcb_end_lookup(dest_tcb);
                return ERR_DEAD;

            }

            let transfer_len = min!(args_kern.long_src.len, message_dest->long_dest.len);
            kmut_2lock(&(get_tcb()->proc->PD.mut), &(proc_recv->PD.mut));

            let page: u32;
            for (page = 0; page < transfer_len; page+=PAGE_SIZE) {
                if (page_transfer(args_kern.long_src.addr + page,
                            message_dest->long_dest.addr + page,
                            &(get_tcb()->proc->PD),
                            &(proc_recv->PD), shared) < 0) {
                    transfer_len = max(0, page - PAGE_SIZE);
                    break;
                }
            }
            kmut_2unlock(&(get_tcb()->proc->PD.mut), &(proc_recv->PD.mut));

            proc_end_lookup(proc_recv);
            message_dest->long_dest.len = transfer_len;
        } 
        rend_signal(short_message, args_kern.short_src.len, rend_node, 0);
    }

    if (copy_to_user(args as *u8, &args_kern as *u8, sizeof(ipc_args)) <
            0) {
        tcb_end_lookup(dest_tcb);
        return ERR_INTERNAL;
    }

    0
}

/* 
 * receives a number of pages up to len_long bytes, mapped in starting with address
 * long_dest.  If a sender is waiting to send, retrieves their info and performs
 * the transfer.  Otherwise, deschedules until a sender becomes available and
 * does the transfer itself.  Returns 0 on success,
 * negative error code on failure
 *
 * Requires long_dest be a userland address
 *
 * long_dest: userspace address where new pages are to be mapped in
 * message: buffer for a message
 * len_short: length of message 
 * timeout: time (in ms) before we give up waiting and return failure
 */
fn ipc_recv(args: *ipc_args) -> i32 {
    let args_kern: ipc_args;
    if (get_tcb()->last_send != null) {
        return ERR_REPLY;
    }

    if (copy_from_user((&args_kern) as *u8, args as *u8, sizeof(ipc_args)) < 0) {
        return ERR_INTERNAL;
    }

    // check to see if addresses all point to user addresses
    if ((!is_user_addr(args_kern.long_dest.addr)  && args_kern.long_dest.len > 0)  ||
            (!is_user_addr(args_kern.short_dest.addr))  && args_kern.short_dest.len > 0)  {
        return ERR_ARGS;  
    }

    // ensures long addresses are page aligned
    if ((!is_addr_aligned(args_kern.long_dest.addr)) && args_kern.long_dest.len > 0)  {
        return ERR_ARGS;  
    }

    // ensures long lengths are multiples of page sizee 
    if (args_kern.long_dest.len%PAGE_SIZE != 0)
    {
        return ERR_ARGS;
    }    

    let message_send: message_node;
    mos_memcpy(&message_send.long_dest, &args_kern.long_dest, sizeof(buffer_t));

    let static_buffer : u32[5];
    let message_recv: *u8 = &static_buffer[0] as *u8;

    let recv_len: u32;
    let err: i32;

    let rend_node = rend_wait(&(get_tcb()->ipc_rend), 
            &message_send as *u32, sizeof(message_node),
            &message_recv as **u32,
            &recv_len, 
            args_kern.timeout, RECV, &err);  

    if (err < 0) {
        return ERR_DEST; 
    }

    // if we have no rend_node and no error, this means the full transfer
    // occurred; clean up and return
    if (rend_node == null) {
        // new length written into message (sender updates this length)
        args_kern.long_dest.len = message_send.long_dest.len;
        args_kern.short_dest.len = recv_len;
        if (copy_to_user((args_kern.short_dest.addr), message_recv as *u8,
                    min(recv_len, args_kern.short_dest.len)) <
                0) {

            return ERR_INTERNAL;
        }
    } else {

        let message_src: *message_node = rend_node->message as *message_node;

        if (message_src->long_dest.len > 0) {

            let transfer_len = min!(message_src->long_src.len, args_kern.long_dest.len);
            kmut_2lock(&(get_tcb()->proc->PD.mut), &(rend_node->tcb->proc->PD.mut));
            let page: u32;
            for (page = 0; page < transfer_len; page+=PAGE_SIZE) {
                if (page_transfer(message_src->long_src.addr + page,
                            args_kern.long_dest.addr + page,
                            &(rend_node->tcb->proc->PD), 
                            &(get_tcb()->proc->PD),
                            message_src-> options&OPTION_SHARE) < 0) {
                    transfer_len = max(0, page - PAGE_SIZE);
                    break;
                }
            }
            args_kern.long_dest.len = transfer_len;
            kmut_2unlock(&(get_tcb()->proc->PD.mut), &(rend_node->tcb->proc->PD.mut));

        }
        if (copy_to_user((args_kern.short_dest.addr),
                    message_src->short_src.addr as *u8,
                    min(message_src->short_src.len, args_kern.short_dest.len)) <
                0) {

            rend_signal(null, 0, rend_node, 0);

            return ERR_INTERNAL;
        }
        if (message_src->short_src.len > 0 &&
                message_src->options&(OPTION_NO_REPLY | OPTION_ASYNC) == 0 &&
                message_src->short_src.len > SHORT_BUFFER_MAX) {
            slub_free(message_src->short_src.addr as *u32,
                    message_src->short_src.len);
        } 


        // if a reply isn't expected, wake up source immediately
        if (message_src->options&OPTION_NO_REPLY != 0 || message_src->options&OPTION_ASYNC != 0) {
            rend_signal(null, 0, rend_node, 0);
        } else {
            assert!(rend_node->tcb != null);
            get_tcb()->last_send = rend_node;
        }

    }

    if (copy_to_user(args as *u8, &args_kern as *u8, sizeof(ipc_args)) <
            0) {
        return ERR_INTERNAL;
    }

    0
}


/* replies to the last sender */
fn reply(message_long: *u8, long_len: u32, message_short: *u32, short_len: u32) -> i32 {
    // TODO how does other guy update on timeout?
    if (get_tcb()->last_send == null) {
        return ERR_REPLY;
    }
    let rend_node = get_tcb()->last_send;
    let message_src: *message_node = rend_node->message as *message_node;


    if (message_src->long_dest.len > 0) {

        let transfer_len = min!(message_src->long_src.len, long_len);
        kmut_2lock(&(get_tcb()->proc->PD.mut), &(rend_node->tcb->proc->PD.mut));
        let page: u32;
        for (page = 0; page < transfer_len; page+=PAGE_SIZE) {
            if (page_transfer(message_long + page,
                        message_src->long_dest.addr + page,
                        &(get_tcb()->proc->PD),
                        &(rend_node->tcb->proc->PD), 0) < 0) {
                transfer_len = max(0, page - PAGE_SIZE);
                break;
            }
        }
        kmut_2unlock(&(get_tcb()->proc->PD.mut), &(rend_node->tcb->proc->PD.mut));

        message_src->long_dest.len = transfer_len;
    }

    let message_kernel = null;
    if (short_len > 0) {
        message_kernel = slub_alloc(short_len);
        if (message_kernel == null || copy_from_user(message_kernel as *u8,
                    message_short as *u8,
                    short_len) < 0) {

            rend_signal(message_kernel, short_len, get_tcb()->last_send, 0);
            return ERR_INTERNAL;
        }
    }

    rend_signal(message_kernel, short_len, get_tcb()->last_send, 0);
    get_tcb()->last_send = null;


    0
}


