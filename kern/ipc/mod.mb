/* ipc/mod.mb
 * Contains implementation of ipc request primitives, such as send and recv 
 * Currently moves unshared pages
 * TODO timeouts, fast path, shared memory, make it possible for
 * synchronous to require replies 
 *
 * Author: Amanda M. Watson
*/

use structures::tcb_dir::*;
use structures::proc_dir::*;
use locks::mutex::*;
use locks::cond::*;
use consts::STATE_DEAD;
use structures::schedule::get_tcb;
use structures::VM::{is_user_addr, page_transfer, log_to_phys, is_addr_aligned};
use locks::rendezvous::*;
use consts::*;
use utils::stdlib::*;
use utils::slab_alloc::*;
use utils::string::*;
use utils::user_mem::*;
use utils::list::*;


extern fn get_cr3() -> u32;

struct message_node {
    long_addr: *u8,
    long_len: u32,
    message: *u8,
    message_len: u32,
}

/*
 * given a valid source buffer and an active tid, send a message to thread
 *  if sending tcb is called while
 * tcb id is waiting, it will perform the transfer itself. If not, it
 * relies on the receiver to perform the
 * transfer.  
 * 
 * The current modes of message passing are the following:
 *  - Long IPC: this is synchronous only.  Sends a page range starting at long_src
 *  and extending to len_long.  If receiver is not currently waiting, thread
 *  deschedules until receiver dies or the receiver sends a reply.
 *  Otherwise, transfers the pages into the receiver's VM and wake up receiver.
 *  Original mapping is removed.  Also sends a short
 *  message message with length len_short.
 * - Short IPC: both synchronous and asynchronous.  A synchronous send acts just
 * like long IPC, but without mapping pages into the VM.  In async, if a
 * receiver is blocked, messages between sender and receiver are exchanged
 * immediately.  if no receiver is available, the message is left for the
 * receiver to pick up later, and sneder returns without a return message. 
 *
 * Requires that long_src be mapped in and be a user address. Requires that
 * len_short be less than PAGE_SIZE
 * Transfer removes mapping ar addr src and stores the frames at long_dest
 * Returns 0 on synchronous success, 1 if message was asynchronous.  A negative error
 * value is returned on error.
 *
 * id: currently the TID of the receiving tcb
 * long_src: the address at which the starting soure frame is mapped
 * message: short message to receiver
 * len_short: length of message
 * len_long: maximum length of transfer in bytes
 * timeout: time (in ms) before we return failure 
 * TODO tell everyone how much was copied
*/
fn ipc_send(id: u32, long_src: *u8, message: *u8, len_long: u32, len_short: *u32, timeout: u32) -> i32 {
    /* temporarily set here, should be set in userspace */
    let sync_option = SEND_SYNC_REPLY;

    if (*len_short >= PAGE_SIZE) {
        return -13;
    }
    if (len_long < 0 || *len_short < 0) {
        return -14;
    }
    /* ensure length is multiple of page */
    if (len_long%PAGE_SIZE != 0) {
        return -15;
    }
    
    let len_short_kernel: u32;
    if (copy_from_user(&len_short_kernel as *u8, len_short as *u8, sizeof(u32))
            < 0) {
        return -16;
    }

    if ((!is_user_addr(long_src) || !is_addr_aligned(long_src)) && len_long != 0) {
        return -3;
    }  

    let kernel_message: *u8 = slub_alloc(len_short_kernel) as *u8;
    if (kernel_message == null && len_short_kernel != 0) {
        return -8;
    }

    if (copy_from_user(kernel_message, message as *u8, len_short_kernel) < 0) {
        return -9;
    }

    /* find the receiving tcb -- it cannot go away as long as we're looking at
     * it */
    let tcb_recv = tcb_lookup(id);
    if (tcb_recv == null) {
        return -1;
    }

    // I think we're ok on the "what if receiver died?" thing, as we have
    // timeouts
    let new_message: message_node;
    new_message.long_addr = long_src as *u8;
    new_message.message = kernel_message;
    new_message.message_len = len_short_kernel;
    new_message.long_len = len_long;

    let message_buf: *u32;
    let message_len: u32;
    let err: u32 = 0;
    let node_recv = rend_wait(&tcb_recv->ipc_rend, &new_message as *u32,
            sizeof(message_node), &message_buf, &message_len, timeout,
            sync_option, &err);

    if (err < 0) {
        assert!(node_recv != null);
        return -18;
    }

    // if it's null, this means we went to sleep before recv, and the transfer
    // was done and we got a reply 
    if (node_recv == null) {
        /* if async, just return.  We did not pick up a message to pass along */
        if (sync_option == SEND_ASYNC) {
            return 1;
        }

        if (message_len > 0) {
            if (copy_to_user(message as *u8, message_buf as *u8,
                        min(len_short_kernel,
                            message_len)) < 0) {
                slub_free(message_buf, message_len);
                tcb_end_lookup(tcb_recv);
                return -10;
            } 
            slub_free(message_buf, message_len);
        } else {
            mos_memset(message, 0, len_short_kernel);
        } 

        tcb_end_lookup(tcb_recv);

        // TODO check message to see if we timed out and should return failure

        len_short_kernel = min(len_short_kernel, message_len);
        if (copy_to_user(len_short as *u8, &len_short_kernel as *u8,
                    sizeof(u32)) < 0) {
            return -16;
        }
        return 0;
    } 

    let m_node = node_recv->message as *message_node;
    let recv_message = m_node->message;
    let recv_len = m_node->message_len;

    assert!(is_user_addr(m_node->long_addr));

    // do a lookup to make sure the address space is still intact
    // TODO figure out if receiver is dead, and proc might no longer exist 
    let proc_recv = tcb_recv->proc;
    // this ensures the proc won't go away as we do things to it

    if (proc_recv == null) {
        printf!("recipiant proc is dead: return with error\n"); 
        tcb_end_lookup(tcb_recv);
        return -4;
    }
    let transfer_len = min!(len_long as u32, m_node->long_len as u32);

    // TODO grab VM lock; check if mem is valid 
    let page: u32;
    for (page = 0; page < transfer_len; page+=PAGE_SIZE) {
        if (page_transfer(long_src + page, m_node->long_addr + page, &(get_tcb()->proc->PD),
                    &(proc_recv->PD), 0) < 0) {
            printf!("problem in page transfer\n");
            tcb_end_lookup(tcb_recv);
            // TODO either put pages back or return a length
            return -5;
        }
    }

    rend_signal(kernel_message as *u32, len_short_kernel, node_recv, 0);

    if (copy_to_user(message as *u8, recv_message, min(len_short_kernel, recv_len)) <
            0) {

        slub_free(recv_message as *u32, recv_len);
        tcb_end_lookup(tcb_recv);

        return -11;
    }
    slub_free(recv_message as *u32, recv_len);

    tcb_end_lookup(tcb_recv);

    len_short_kernel = min(len_short_kernel, message_len);
    if (copy_to_user(len_short as *u8, &len_short_kernel as *u8, sizeof(u32)) <
            0) {
        return -16;
    }

    0
}

/* 
 * receives a number of pages up to len_long bytes, mapped in starting with address
 * long_dest.  If a sender is waiting to send, retrieves their info and performs
 * the transfer.  Otherwise, deschedules until a sender becomes available and
 * does the transfer itself.  Returns 0 on success,
 * negative error code on failure
 *
 * Requires long_dest be a userland address
 *
 * long_dest: userspace address where new pages are to be mapped in
 * message: buffer for a message
 * len_short: length of message 
 * timeout: time (in ms) before we give up waiting and return failure
 */
fn ipc_recv(long_dest: *u8, message: *u8, len_long: u32, len_short: *u32, timeout: u32) -> i32 {

    /* if we haven't replied to our last message, reuturn with error */
    if (get_tcb()->last_send != null) {
        return -12;
    }

    /* ensure length is multiple of page */
    if (len_long%PAGE_SIZE != 0) {
        return -15;
    }

    if ((!is_user_addr(long_dest) || !is_addr_aligned(long_dest)) && len_long != 0) {
        return -3;
    } 

    let kernel_len_short: u32;
    if (copy_from_user(&kernel_len_short as *u8, len_short as *u8, sizeof(u32)) < 0) {
        return -16;
    }

    // this gets encapsulated in user mem validation
    let kernel_message: *u8 = slub_alloc(kernel_len_short) as *u8;
    if (kernel_message == null && kernel_len_short != 0) {
        return -8;
    }
    if (copy_from_user(kernel_message, message as *u8, kernel_len_short) < 0) {
        return -9;
    }

    let tcb_recv = get_tcb();
    assert!(tcb_recv != null);

    let new_message : message_node;
    new_message.long_addr = long_dest;
    new_message.message = null;
    new_message.message_len = 0;
    new_message.long_len = len_long;

    let message_buf: *u32;
    let message_len: u32;
    let err: u32 = 0;
    let node_send = rend_wait(&tcb_recv->ipc_rend, &new_message as *u32,
            sizeof(message_node), &message_buf, &message_len, timeout, RECV,
            &err);

    if (err < 0) {
        assert!(node_send != null);
        return -18;
    }

    if (node_send == null) {
        // TODO make sure this can't be a success case; only for timeouts
        if (message_buf == null || message_len <= 0) {
            return -2
        }

        if (copy_to_user(message as *u8, message_buf as *u8, min(message_len,
                        kernel_len_short)) < 0) {
            //slub_free(message_buf, message_len);
            return -12;

        }

        //slub_free(message_buf, message_len);

        kernel_len_short = min(message_len, kernel_len_short);
        if (copy_to_user(len_short as *u8, &kernel_len_short as *u8,
                    sizeof(u32)) < 0) {
            return -16;
        }

        return 0;
    }

    let m_node = node_send->message as *message_node;
    let send_message = m_node->message;
    let send_len = m_node->message_len;

    assert!(is_user_addr(m_node->long_addr) || m_node->long_len == 0);

    let transfer_len = min!(len_long as u32, m_node->long_len as u32);

    let page: u32;
    for (page = 0; page < transfer_len; page+=PAGE_SIZE) {
        if (page_transfer(m_node->long_addr + page, long_dest + page,
                    &(node_send->tcb->proc->PD), &(get_tcb()->proc->PD), 0) < 0) {

            printf!("problem in page transfer\n");
            // TODO either put pages back or return a length
            return -1;
        }
    }

    // TODO encapsulate in rend
    if (node_send->option != SEND_ASYNC) {
        get_tcb()->last_send = node_send;
    }

    // this gets encapsulated in user mem validation
    if (copy_to_user(message as *u8, send_message, min(kernel_len_short, send_len)) <
            0) {
        slub_free(send_message as *u32, send_len);
        return -11;
    }

    slub_free(send_message as *u32, send_len);

    kernel_len_short = min(message_len, kernel_len_short);
    if (copy_to_user(len_short as *u8, &kernel_len_short as *u8,
                sizeof(u32)) < 0) {
        return -16;
    }

    0
}


/* replies to the last sender */
fn reply(message: *u32, message_len: u32) -> i32 {
    let tcb = get_tcb();

    // TODO encapsulate in rend
    if (tcb->last_send == null) {
        return -1;
    }
    let kernel_message: *u8 = null;
    if (message_len != 0) {
        kernel_message = slub_alloc(message_len) as *u8;
        if (kernel_message == null && message_len != 0) {
            return -8;
        }
        if (copy_from_user(kernel_message, message as *u8, message_len) < 0) {
            return -9;
        }
    }

    rend_signal(kernel_message as *u32, message_len, tcb->last_send, 0);
    0
}


